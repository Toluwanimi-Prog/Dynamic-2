{
  "hash": "c47618714b82b47ce4079585d7a18d05",
  "result": {
    "engine": "knitr",
    "markdown": "# Derrick Baruga\n\nThis page contains all of Derrick Baruga's submissions this semester organized into different sections.\n\n## Wednesday\n\n### Week 1\n\n### Week 2\n\nI am using the air-quality dataset, which contains air quality measurements collected over several months, specifically from May to September. The dataset includes the following variables: **Ozone**, **Solar.R** (solar radiation), **Wind** (wind speed in miles per hour), **Temp** (temperature in degrees Fahrenheit), **Month** (ranging from 5 for May to 9 for September), and **Day** (ranging from 1 to 31).\n\n**Cleaning The Data**\n\nI used Excel to clean the data by removing all rows with NA values and performed an exploratory analysis to identify patterns, trends, and potential relationships between these variables over the specified months and days.\n\n**Ozone Histogram**\n\n![](hist_ozone_baruga.png)\n\nThe Ozone histogram has its highest values on the left and then curbs off to the right, it indicates a right-skewed distribution.\n\nThis means:\n\n-   Most ozone values are low, with a few much higher values creating a long tail on the right.\n-   The skewness could result from natural variability, pollution events, or weather conditions affecting ozone levels.\n\n**Scatter Plot Ozone vs Temperature**\n\n![](scatter_ozone_vs_temp_baruga.png)\n\n-   A scatter plot of ozone (x-axis) vs. temperature (y-axis) with a slight positive correlation means that higher ozone levels tend to be associated with higher temperatures. However, the relationship is weak, suggesting other factors (like wind, humidity, or pollution) also affect ozone levels and temperature.\n\n**Pivot Table Average Ozones Per Day & Month (Redacted)**\n\n![](pivot4_table_baruga.png)\n\n-   The pivot table presents daily sums for multiple variables (Temperature, Wind, Solar Radiation, and Ozone) over several months (May to September). The days with the highest ozone averages are Day 25 in May (with an Ozone value of 5-65) and July (with an Ozone value of 7-74), with other notable days being Day 29 and Day 30 in May. The months with the highest ozone levels are May, which shows several days with high averages, and July, particularly on Days 25 and 29.\n\n**Pivot Chart Average Ozones Per Day & Month**\n\n![](pivot4_chart_baruga.png)\n\n**Pivot Table of Variation in Solar Radiation and Temperature**\n\n-   The pivot chart provide a visual representation of how these variables change over time. The histogram shows day-to-day variations in ozone levels, with the highest concentrations occurring in the summer months. Notably, there is a pronounced peak around July 25th, indicating exceptionally high ozone levels on that day. May and July both have elevated levels, with smaller peaks around May 29th and 30th, but July 25th stands out as the most significant. Overall, the chart confirms that ozone levels are highest in July, particularly around the 25th.\n\n![](pivot1_table_baruga.png)\n\n-   The pivot table shows daily sums for Solar Radiation and Temperature over a month. There is noticeable daily variation, with high values on days like 18, 19, and 29, indicating intense sunlight and warmer temperatures, and lower values on days like 23 and 27, reflecting cooler conditions. The grand totals summarize the entire month, with 20,513 for Solar Radiation and 8,635 for Temperature. Overall, the table captures daily fluctuations in weather conditions.\n\n\\*\\* Pivot Chart of Pivot Chart of Variation in Solar Radiation and Temperature\\*\\*\n\n![](pivot1_chart_baruga.png)\n\n-   The bar chart shows daily sums of Solar Radiation (blue) and Temperature (orange) over 31 days. High Solar Radiation is notable on days like 9, 13, 16, 18, 19, and 29, with values exceeding 1,000. Temperature values are generally lower, mostly below 400, with higher values on days like 9 and 18. There are significant day-to-day variations, with some days showing high Solar Radiation but lower temperatures (e.g., Day 13). The chart captures daily fluctuations and highlights days with extreme values.\n\n**Pivot Table**\n\n![](pivot2_table_baruga.png)\n\n-   The pivot table displays the average temperature (`Average of Temp`) and ozone levels (`Average of Ozone`) for days labeled 5 to 9, showing average temperatures ranging from 66.46 to 83.88, with the highest temperatures recorded on days 7 and 8. Ozone levels vary significantly, from a low of 24.13 on Day 5 to a high of 60.00 on Day 8. Overall, the average temperature for the period is 77.79, and the average ozone level is 42.10, reflecting moderate temperatures with variable ozone levels across these days and highlighting daily fluctuations in both metrics.\n\n**Pivot Chart**\n\n![](pivot2_chart_baruga.png)\n\n-   The bar chart illustrates the average temperature (in blue) and average ozone levels (in orange) for days 5 to 9, showing that temperatures remain relatively high throughout, ranging from around 66 on Day 5 to approximately 84 on Days 7 and 8. Ozone levels start low on Day 5 (around 24), rise significantly by Days 7 and 8 (around 59-60), and then decrease slightly on Day 9 (around 31). The chart suggests a potential correlation between higher temperatures and elevated ozone levels, as Days 7 and 8, which have the highest temperatures, also show the highest average ozone levels, indicating noticeable variability over the period.\n\n### Week 3\n\n[Wednesday Dashboard](https://public.tableau.com/views/Book1_17262574551960/Dashboard1?:language=en-US&publish=yes&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link)\n\n1.  **Histogram of Avg REM Sleep**:\n\n-   This bar chart shows the average REM sleep time categorized by both conservation status (like \"cd,\" \"en,\" \"lc\") and dietary habits (carnivore, herbivore, omnivore, etc.).\n\n-   The distribution indicates variations in REM sleep depending on these factors. For example, certain categories, such as carnivores with an \"lc\" (least concern) conservation status, seem to have higher average REM sleep.\n\n2.  **Stacked Bar Chart of Vore vs. REM Sleep**:\n\n-   This chart displays the breakdown of average REM sleep across different dietary categories and their conservation statuses, further subdivided by animal order.\n\n-   It provides detailed insights into how sleep patterns differ based on dietary habits and animal groups. For instance, \"Carnivora\" under different conservation statuses like \"lc\" and \"domesticated\" shows varying REM sleep levels.\n\n3.  **Packed Bubble Chart of Order**:\n\n-   The chart uses bubbles to represent different orders of animals, with the size of each bubble possibly corresponding to the number of species or the average REM sleep within that order.\n\n-   Colors differentiate various dietary habits (\"vore\"), showing how different animal orders fall into categories such as carnivores, herbivores, omnivores, etc.\n\n4.  **Treemap of Order**:\n\n-   This chart breaks down the animal orders into smaller rectangles, where the size of each rectangle may indicate the average REM sleep, the number of species, or another quantitative measure.\n\n-   The colors correspond to different orders, offering a visual overview of how these orders compare in the measured metric.\n\n## Friday - Midterm Projects\n\n### Week 1\n\n### Week 2\n\ntitle: \"Midterm_Project_Baruga\" subtitle: \"Data.gov: Warehouse and Retail Sales\" format: html ---\n\n**Context of the Dataset**\n\n-   **Title**: Warehouse and Retail Sales\n-   **Link**: [Download the CSV dataset](https://data.montgomerycountymd.gov/api/views/v76h-r7br/rows.csv?accessType=DOWNLOAD)\n\nThe *Warehouse and Retail Sales* dataset provides a comprehensive view of sales activities in Montgomery County, Maryland, by capturing data from various warehouse and retail operations. This dataset was collected through a combination of direct reporting from businesses, automated sales tracking systems, and regional sales surveys. The data encompasses a range of sales metrics, including volume and product categories, to offer insights into business performance across different types of establishments.\n\nThe dataset includes the following variables:\n\n-   Row Labels: Categories or identifiers used to organize and classify the sales data.\n-   BEER: Sales volume for beer products.\n-   DUNNAGE: Sales volume for dunnage products, which are materials used to protect goods during transportation.\n-   KEGS: Sales volume for kegs, typically used for storing and transporting beverages.\n-   LIQUOR: Sales volume for liquor products.\n-   NON-ALCOHOL: Sales volume for non-alcoholic beverages.\n-   REF: Sales volume for refrigeration supplies or products.\n-   STR_SUPPLIES: Sales volume for store supplies, which may include various retail essentials.\n-   WINE: Sales volume for wine products.\n-   Grand Total: The total sales volume across all categories combined.\n\n\\*\\* Data Cleaning and Preparation\\*\\*\n\n1.  **Import the Data**: I downloaded and loaded the CSV file into Excel.\n2.  **Check for Missing Values**: My preferred method for handling NAs is by highlighting them and deselecting them using the filter tool for each column as I feel most thorough by doing that. Using the following steps \"Select the entire dataset, go to Home \\> Conditional Formatting \\> Highlight Cells Rules \\> Blanks to highlight all blank cells.\"\n3.  **Data Formatting**: Formated any columns that need specific data types (such as dates as date format, numbers as currency or percentage).\n\n-   Specifically I used the =TEXTJOIN(\"-\", TRUE, A2, B2) function to join column one (YYYY) and column two (MM) into a new column called TIME\n\n![](summary_screenshot.png)\n\n**Visualization**\n\n**Pivot Tables YT link:** https://www.youtube.com/watch?v=qu-AK0Hv0b4\n\n-   Pivot tables are a powerful and user friendly (drag and drop) summary statistic/visualisation tool that I used to get an intial feel of my data.\n\nHere are the findings:\n\n**Warehouse Sales/Expenses of Beverages Over Time (2017 - 2020)**\n\n![](pivot_table_baruga.png)\n\n-   **Overall Sales**: ***BEER*** has the highest sales, followed by ***WINE***, while ***DUNNAGE***, ***REF***, and ***STR_SUPPLIES*** have negative or minimal sales, which makes sense as refers to materials used to protect goods during shipping and handling, such as packing materials or cushioning, and as such is an expense to the business leading to its negative output on revenue.\n-   **Monthly Trends**: ***BEER*** sales vary significantly, with large peaks and drops. ***WINE*** sales are more stable but still show some fluctuation. ***KEGS*** and ***LIQUOR*** show positive but lower sales.\n\n**Histogram of Warehouse Sales/Expenses of Beverages Over Time**\n\n![](pivot_chart.png)\n\n-   Here is a histogram representation of the aforementioned summary pivot tables. There appears to be a disporportionate amount of ***BEER*** bought for warehouses. But that could be due to the fact that beers are sold in packs and so single unit quantity has skyrocketed in order to make a \"12 pack\" that will later count as one unit sold at retail.\n\n**Retail Sales of Beverages Over Time (2017 - 2020)**\n\n![](pivot2_table.png)\n\n-   **Product Types**: Includes Beer, Liquor, Wine, Non-Alcoholic beverages, and others, with substantial sales figures for Beer, Liquor, and Wine.\n-   **Sales Trends**: Liquor leads with \\$802,691.43 in total sales, followed by Wine (\\$746,498.59) and Beer (\\$574,220.53). Recent months show higher sales for Beer and Wine.\n-   **Low Sales Categories**: Items like Dunnage and Kegs have negligible or zero sales.\n-   **Overall Total**: Total sales across all products amount to \\$2,160,899.37, highlighting overall retail activity.\n\n**Histogram of Retail Sales of Beverages Over Time**\n\n![](pivot_chart_2.png)\n\n-   Once we come to the retail side of things we see that ***WINE*** and ***LIQUOR*** are clear best sellers. It is shown hower that ***LIQUOR*** has begun to overtake ***WINE*** in retail sales.\n\n**Summary**\n\nFrom the \"Warehouse and Retail Sales\" dataset, I found that Beer leads in warehouse sales, with notable fluctuations due to bulk packaging, while Wine and Liquor have more stable sales. Retail sales show Liquor as the top seller, recently surpassing Wine, with Beer also performing strongly but declining. The histograms illustrate high Beer volume in warehouses and a shift in retail dominance from Wine to Liquor. Next, I will analyze seasonal trends, create advanced visualizations for deeper insights, and finalize the report with comprehensive findings and recommendations. \\### Week 2 \\### Week 3\n\n### Week 3\n\n[Wednesday Dashboard](https://public.tableau.com/views/Book1_17262574551960/Dashboard2?:language=en-US&publish=yes&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link)\n\n**1.Histogram of Avg Sales Over Time:**\n\n-   This stacked bar chart displays the average retail sales from June 2017 to September 2020, broken down by different item types such as \"BEER,\" \"WINE,\" \"LIQUOR,\" \"NON-ALCOHOL,\" and others.\n\n-   The chart shows monthly fluctuations in sales, with noticeable peaks around December 2017 and July 2020. This suggests seasonal effects or particular periods of high demand for certain items.\n\n-   The different colors represent various item types, indicating the contribution of each category to the total sales in each month. For example, \"LIQUOR\" and \"NON-ALCOHOL\" seem to contribute significantly to the total sales during peak months.\n\n**2. Area Chart: Avg Sales vs. Transfers:**\n\n-   This plot consists of two layered area charts: the top one shows average retail transfers, and the bottom one shows average retail sales over the same period (June 2017 to September 2020).\n\n-   Both charts use colors to represent different item types, revealing how each type contributes to overall sales and transfers.\n\n-   The charts indicate that the trends in transfers often align with the sales trends, which suggests a correlation between the quantity of goods transferred and the sales performance.\n\n-   Peaks and troughs in the charts could indicate seasonal variations, inventory management strategies, or market demand shifts for various items.\n\n**3. Packed Bubble Chart of Avg Sales Over Time:**\n\n-   This chart visualizes average sales using bubbles, where the size of each bubble reflects the volume of sales, and the color represents different item types.\n\n-   Larger bubbles correspond to higher sales, indicating which item types have the greatest impact on sales over time.\n\n-   The variety of bubble sizes and colors reveals the diversity in item types and their varying sales performance.\n\n**4. Treemap of Retail Transfers vs. Retail Sales:**\n\n-   The treemap displays retail sales data, with each rectangle representing different categories (\"NON-ALCOHOL,\" \"BEER,\" \"LIQUOR,\" \"REF,\" etc.) and specific years (2017, 2019, 2020).\n\n-   The size of each rectangle corresponds to the magnitude of sales, and the color shading indicates the average retail sales, with darker shades representing higher sales.\n\n-   This visualization shows how different item types and their sales vary in significance. For example, \"NON-ALCOHOL\" items appear to have a prominent share, especially in 2020.\n\n### Week 8 \n\nThis contains Derrick Week 8 Submissions \n\nI chose the dataset “Federal, State, and Local Government Transportation-Related Revenues and Expenditures, Fiscal Year” provides financial data on transportation-related revenues and expenditures across federal, state, and local governments in the United States, measured in millions of current dollars. It includes detailed information on government revenues from user charges and taxes specifically allocated for transportation programs, as well as expenditures in this sector over multiple fiscal years. The dataset excludes general fund revenues and borrowing proceeds, focusing on own-source revenues.\n\nThe data is sourced from the U.S. Department of Transportation’s Bureau of Transportation Statistics (BTS) and is published under the National Transportation Statistics Table 3-29, available on the BTS website. This dataset is instrumental for analyzing transportation funding and government spending trends at various governmental levels.\n\n**Dataset Exploration**\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(readxl)  # To read Excel files\nlibrary(dplyr)   # For data manipulation\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2) # For visualization\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.3.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Task 1: Load the dataset\nfile_path <-  \"C:/Users/toluf/OneDrive/Desktop/Dynamic 2/Archive 2/table_03_29_061324.xlsx\" \n\n\ndataset <- read_excel(file_path, sheet = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n• `` -> `...2`\n• `` -> `...3`\n• `` -> `...4`\n• `` -> `...5`\n• `` -> `...6`\n• `` -> `...7`\n• `` -> `...8`\n• `` -> `...9`\n• `` -> `...10`\n• `` -> `...11`\n• `` -> `...12`\n• `` -> `...13`\n• `` -> `...14`\n• `` -> `...15`\n• `` -> `...16`\n```\n\n\n:::\n\n```{.r .cell-code}\n# Explore the structure of the dataset\nstr(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [17 × 16] (S3: tbl_df/tbl/data.frame)\n $ Table 3-29:  Federal, State, and Local Government Transportation-Related Revenues and Expenditures, Fiscal Year (millions of current dollars): chr [1:17] NA \"Total government revenues\" \"Federal\" \"State and local\" ...\n $ ...2                                                                                                                                         : num [1:17] 2007 163884 53967 109917 268843 ...\n $ ...3                                                                                                                                         : num [1:17] 2008 163222 52102 111120 284343 ...\n $ ...4                                                                                                                                         : num [1:17] 2009 157684 47287 110397 300267 ...\n $ ...5                                                                                                                                         : num [1:17] 2010 160472 47244 113227 303516 ...\n $ ...6                                                                                                                                         : num [1:17] 2011 171324 50310 121014 303784 ...\n $ ...7                                                                                                                                         : num [1:17] 2012 179173 54473 124699 314024 ...\n $ ...8                                                                                                                                         : num [1:17] 2013 174527 50686 123841 309276 ...\n $ ...9                                                                                                                                         : num [1:17] 2014 183537 54161 129376 324000 ...\n $ ...10                                                                                                                                        : num [1:17] 2015 194195 56714 137481 329551 ...\n $ ...11                                                                                                                                        : num [1:17] 2016 194420 57279 137141 339439 ...\n $ ...12                                                                                                                                        : num [1:17] 2017 203528 57628 145900 355374 ...\n $ ...13                                                                                                                                        : num [1:17] 2018 213373 60035 153338 370884 ...\n $ ...14                                                                                                                                        : num [1:17] 2019 228293 62142 166150 384211 ...\n $ ...15                                                                                                                                        : num [1:17] 2020 200560 53719 146842 404177 ...\n $ ...16                                                                                                                                        : num [1:17] 2021 196163 53503 142660 398764 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check for missing values\nsummary(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Table 3-29:  Federal, State, and Local Government Transportation-Related Revenues and Expenditures, Fiscal Year (millions of current dollars)\n Length:17                                                                                                                                    \n Class :character                                                                                                                             \n Mode  :character                                                                                                                             \n                                                                                                                                              \n                                                                                                                                              \n                                                                                                                                              \n                                                                                                                                              \n      ...2             ...3             ...4             ...5       \n Min.   :  2007   Min.   :  2008   Min.   :  2009   Min.   :  2010  \n 1st Qu.: 42052   1st Qu.: 45088   1st Qu.: 42913   1st Qu.: 43445  \n Median : 81942   Median : 81611   Median : 82167   Median : 85120  \n Mean   :114380   Mean   :118497   Mean   :121481   Mean   :123375  \n 3rd Qu.:183756   3rd Qu.:186542   3rd Qu.:185882   3rd Qu.:188221  \n Max.   :268843   Max.   :284343   Max.   :300267   Max.   :303516  \n NA's   :9        NA's   :9        NA's   :9        NA's   :9       \n      ...6             ...7             ...8             ...9       \n Min.   :  2011   Min.   :  2012   Min.   :  2013   Min.   :  2014  \n 1st Qu.: 46028   1st Qu.: 49049   1st Qu.: 46068   1st Qu.: 48811  \n Median : 87711   Median : 91481   Median : 90561   Median : 94066  \n Mean   :125829   Mean   :130834   Mean   :128363   Mean   :134480  \n 3rd Qu.:196144   3rd Qu.:204692   3rd Qu.:200162   3rd Qu.:210463  \n Max.   :303784   Max.   :314024   Max.   :309276   Max.   :324000  \n NA's   :9        NA's   :9        NA's   :9        NA's   :9       \n     ...10            ...11            ...12            ...13       \n Min.   :  2015   Min.   :  2016   Min.   :  2017   Min.   :  2018  \n 1st Qu.: 50609   1st Qu.: 51743   1st Qu.: 52507   1st Qu.: 53460  \n Median : 97294   Median : 98386   Median :101765   Median :107654  \n Mean   :138327   Mean   :141171   Mean   :147181   Mean   :154063  \n 3rd Qu.:219960   3rd Qu.:221892   3rd Qu.:232204   3rd Qu.:244318  \n Max.   :329551   Max.   :339439   Max.   :355374   Max.   :370884  \n NA's   :9        NA's   :9        NA's   :9        NA's   :9       \n     ...14            ...15            ...16       \n Min.   :  2019   Min.   :  2020   Min.   :  2021  \n 1st Qu.: 55380   1st Qu.: 50734   1st Qu.: 50281  \n Median :115440   Median :113377   Median :113880  \n Mean   :161470   Mean   :161426   Mean   :159622  \n 3rd Qu.:258499   3rd Qu.:241019   3rd Qu.:236660  \n Max.   :384211   Max.   :404177   Max.   :398764  \n NA's   :9        NA's   :9        NA's   :9       \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check for duplicates\nduplicated_rows <- dataset[duplicated(dataset), ]\n\n# Rename columns for easier access\ncolnames(dataset) <- c(\"Description\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \n                       \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \n                       \"2019\", \"2020\", \"2021\")\n\n# Get an overview of the data\nhead(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 16\n  Description     `2007` `2008` `2009` `2010` `2011` `2012` `2013` `2014` `2015`\n  <chr>            <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 <NA>            2.01e3 2.01e3 2.01e3 2.01e3 2.01e3 2.01e3 2.01e3 2.01e3 2.02e3\n2 Total governme… 1.64e5 1.63e5 1.58e5 1.60e5 1.71e5 1.79e5 1.75e5 1.84e5 1.94e5\n3 Federal         5.40e4 5.21e4 4.73e4 4.72e4 5.03e4 5.45e4 5.07e4 5.42e4 5.67e4\n4 State and local 1.10e5 1.11e5 1.10e5 1.13e5 1.21e5 1.25e5 1.24e5 1.29e5 1.37e5\n5 Total governme… 2.69e5 2.84e5 3.00e5 3.04e5 3.04e5 3.14e5 3.09e5 3.24e5 3.30e5\n6 State and loca… 2.43e5 2.57e5 2.70e5 2.71e5 2.71e5 2.81e5 2.77e5 2.91e5 2.97e5\n# ℹ 6 more variables: `2016` <dbl>, `2017` <dbl>, `2018` <dbl>, `2019` <dbl>,\n#   `2020` <dbl>, `2021` <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n**Data Cleaning**\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Task 2: Data Cleaning\n\n# Remove duplicates (if any)\ndataset <- dataset %>% distinct()\n\n# Handle missing values (remove rows with missing values or impute with mean/median if necessary)\n# Option 1: Remove rows with missing values\ncleaned_dataset <- na.omit(dataset)\n\n# Option 2: Impute missing values with median (can also use mean if more appropriate)\ncleaned_dataset <- dataset %>% \n  mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\n# Check for inconsistencies or outliers using boxplots (visual inspection)\nboxplot(cleaned_dataset[, sapply(cleaned_dataset, is.numeric)], main = \"Boxplots for Numeric Variables\")\n```\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Remove rows where the Description column is NA\ncleaned_dataset <- dataset %>% filter(!is.na(Description))\n\n# Convert year columns to numeric\ncleaned_dataset[, -1] <- lapply(cleaned_dataset[, -1], as.numeric)\n\n# Check if the conversion was successful\nstr(cleaned_dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [14 × 16] (S3: tbl_df/tbl/data.frame)\n $ Description: chr [1:14] \"Total government revenues\" \"Federal\" \"State and local\" \"Total government expenditures\" ...\n $ 2007       : num [1:14] 163884 53967 109917 268843 243373 ...\n $ 2008       : num [1:14] 163222 52102 111120 284343 256501 ...\n $ 2009       : num [1:14] 157684 47287 110397 300267 270478 ...\n $ 2010       : num [1:14] 160472 47244 113227 303516 271470 ...\n $ 2011       : num [1:14] 171324 50310 121014 303784 270602 ...\n $ 2012       : num [1:14] 179173 54473 124699 314024 281248 ...\n $ 2013       : num [1:14] 174527 50686 123841 309276 277065 ...\n $ 2014       : num [1:14] 183537 54161 129376 324000 291241 ...\n $ 2015       : num [1:14] 194195 56714 137481 329551 297255 ...\n $ 2016       : num [1:14] 194420 57279 137141 339439 304305 ...\n $ 2017       : num [1:14] 203528 57628 145900 355374 318231 ...\n $ 2018       : num [1:14] 213373 60035 153338 370884 337150 ...\n $ 2019       : num [1:14] 228293 62142 166150 384211 349118 ...\n $ 2020       : num [1:14] 200560 53719 146842 404177 362396 ...\n $ 2021       : num [1:14] 196163 53503 142660 398764 358148 ...\n```\n\n\n:::\n:::\n\n\n\n\n\n\n**Descriptive Statistics**\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate summary statistics for numeric columns\ndescriptive_stats <- cleaned_dataset %>%\n  summarise(across(where(is.numeric), list(\n    mean = ~ mean(., na.rm = TRUE),\n    median = ~ median(., na.rm = TRUE),\n    std_dev = ~ sd(., na.rm = TRUE)\n  )))\n\n# View the descriptive statistics\nprint(descriptive_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 45\n  `2007_mean` `2007_median` `2007_std_dev` `2008_mean` `2008_median`\n        <dbl>         <dbl>          <dbl>       <dbl>         <dbl>\n1     130433.       109917.         97623.     135138.       111120.\n# ℹ 40 more variables: `2008_std_dev` <dbl>, `2009_mean` <dbl>,\n#   `2009_median` <dbl>, `2009_std_dev` <dbl>, `2010_mean` <dbl>,\n#   `2010_median` <dbl>, `2010_std_dev` <dbl>, `2011_mean` <dbl>,\n#   `2011_median` <dbl>, `2011_std_dev` <dbl>, `2012_mean` <dbl>,\n#   `2012_median` <dbl>, `2012_std_dev` <dbl>, `2013_mean` <dbl>,\n#   `2013_median` <dbl>, `2013_std_dev` <dbl>, `2014_mean` <dbl>,\n#   `2014_median` <dbl>, `2014_std_dev` <dbl>, `2015_mean` <dbl>, …\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the distribution for one of the years, e.g., 2021\nhist(cleaned_dataset$`2021`, \n     main = \"Distribution of Transportation-Related Revenues/Expenditures for 2021\", \n     xlab = \"Revenue/Expenditure (Millions)\", \n     breaks = 20)\n```\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Example: Create a correlation matrix to identify relationships between variables\ncor_matrix <- cor(cleaned_dataset %>% select(where(is.numeric)), use = \"complete.obs\")\nprint(cor_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          2007      2008      2009      2010      2011      2012      2013\n2007 1.0000000 0.9992200 0.9957372 0.9955944 0.9982996 0.9987699 0.9982597\n2008 0.9992200 1.0000000 0.9985997 0.9984952 0.9995968 0.9998451 0.9995891\n2009 0.9957372 0.9985997 1.0000000 0.9999387 0.9988945 0.9988467 0.9989537\n2010 0.9955944 0.9984952 0.9999387 1.0000000 0.9990215 0.9989126 0.9990930\n2011 0.9982996 0.9995968 0.9988945 0.9990215 1.0000000 0.9999100 0.9999483\n2012 0.9987699 0.9998451 0.9988467 0.9989126 0.9999100 1.0000000 0.9999287\n2013 0.9982597 0.9995891 0.9989537 0.9990930 0.9999483 0.9999287 1.0000000\n2014 0.9985962 0.9997517 0.9988848 0.9989586 0.9999355 0.9999718 0.9999746\n2015 0.9993482 0.9993916 0.9970608 0.9971437 0.9994229 0.9995078 0.9994220\n2016 0.9988899 0.9997125 0.9983948 0.9984766 0.9999131 0.9999101 0.9998824\n2017 0.9986660 0.9992946 0.9977322 0.9978360 0.9997184 0.9995628 0.9995986\n2018 0.9989272 0.9994124 0.9977061 0.9977523 0.9995666 0.9995920 0.9996157\n2019 0.9987879 0.9985028 0.9957926 0.9959504 0.9987484 0.9987679 0.9988146\n2020 0.9911542 0.9955344 0.9990016 0.9991752 0.9966571 0.9963679 0.9968956\n2021 0.9894631 0.9942597 0.9982861 0.9984816 0.9953152 0.9951339 0.9956841\n          2014      2015      2016      2017      2018      2019      2020\n2007 0.9985962 0.9993482 0.9988899 0.9986660 0.9989272 0.9987879 0.9911542\n2008 0.9997517 0.9993916 0.9997125 0.9992946 0.9994124 0.9985028 0.9955344\n2009 0.9988848 0.9970608 0.9983948 0.9977322 0.9977061 0.9957926 0.9990016\n2010 0.9989586 0.9971437 0.9984766 0.9978360 0.9977523 0.9959504 0.9991752\n2011 0.9999355 0.9994229 0.9999131 0.9997184 0.9995666 0.9987484 0.9966571\n2012 0.9999718 0.9995078 0.9999101 0.9995628 0.9995920 0.9987679 0.9963679\n2013 0.9999746 0.9994220 0.9998824 0.9995986 0.9996157 0.9988146 0.9968956\n2014 1.0000000 0.9995464 0.9999361 0.9996537 0.9997081 0.9989196 0.9965499\n2015 0.9995464 1.0000000 0.9997745 0.9997836 0.9998933 0.9997817 0.9937224\n2016 0.9999361 0.9997745 1.0000000 0.9998653 0.9998327 0.9992473 0.9956970\n2017 0.9996537 0.9997836 0.9998653 1.0000000 0.9998272 0.9994682 0.9948384\n2018 0.9997081 0.9998933 0.9998327 0.9998272 1.0000000 0.9996941 0.9948117\n2019 0.9989196 0.9997817 0.9992473 0.9994682 0.9996941 1.0000000 0.9924318\n2020 0.9965499 0.9937224 0.9956970 0.9948384 0.9948117 0.9924318 1.0000000\n2021 0.9952895 0.9920435 0.9942353 0.9930950 0.9932298 0.9905877 0.9998203\n          2021\n2007 0.9894631\n2008 0.9942597\n2009 0.9982861\n2010 0.9984816\n2011 0.9953152\n2012 0.9951339\n2013 0.9956841\n2014 0.9952895\n2015 0.9920435\n2016 0.9942353\n2017 0.9930950\n2018 0.9932298\n2019 0.9905877\n2020 0.9998203\n2021 1.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize correlations (optional)\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.92 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\ncorrplot(cor_matrix, method = \"circle\")\n```\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Jupyter Notebooks\n\n### Week 4\n\n\n**Markdown title**\n\nMarkdown Lists:\n\n- Item 1\n- Item 2\n- Item 3\n\nEnumarated list\n\n1. Hola\n2. Hi\n3. Namaste\n\nWe can do **bold**, or *italic*\n\n\n```python\n# Importing Numpy with nickname np\nimport numpy as np\nnp.absolute(-1)\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n```\n\n    [1 2 3 4 5]\n    \n\n\n```python\n# Lists are native to python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n```\n\n    [1, 2, 3, 4, 5]\n    \n\n\n```python\n# Dataframes, so we need pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n```\n\n       Ozone  Temp\n    0     41    67\n    1     36    72\n    2     12    74\n    \n\n**4. Loading csv files**\n\nTo load .csv files into a 'DataFrame', we use pandas function read_csv\n\n\n```python\ndf = pd.read_csv('/Users/derrickmarkbavaudbaruga/Documents/fall 2024/CSC 477/Week 2/airquality_datasets.csv')\n# Summary of the dataset\nprint(df.info())\nprint(df.describe())\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 153 entries, 0 to 152\n    Data columns (total 6 columns):\n     #   Column   Non-Null Count  Dtype  \n    ---  ------   --------------  -----  \n     0   Ozone    116 non-null    float64\n     1   Solar.R  146 non-null    float64\n     2   Wind     153 non-null    float64\n     3   Temp     153 non-null    int64  \n     4   Month    153 non-null    int64  \n     5   Day      153 non-null    int64  \n    dtypes: float64(3), int64(3)\n    memory usage: 7.3 KB\n    None\n                Ozone     Solar.R        Wind        Temp       Month         Day\n    count  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\n    mean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\n    std     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\n    min      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n    25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n    50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n    75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\n    max    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\n    \n\n\n```python\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n\n    \n![png](week4_baruga_python_files/week4_baruga_python_6_0.png)\n    \n\n\n\n```python\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n\n    \n![png](week4_baruga_python_files/week4_baruga_python_7_0.png)\n    \n\n\n\n```python\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n```\n\n\n    \n![png](week4_baruga_python_files/week4_baruga_python_8_0.png)\n    \n\n\n\n```python\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n```\n\n\n    \n![png](week4_baruga_python_files/week4_baruga_python_9_0.png)\n    \n\n\n\n\n### Week 5\n\n\n\n\n**1. Introduction to Plotnine**\n\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\n\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# !pip install plotnine\n```\n:::\n\n\n\n\n\n**2. The Grammar of Graphics**\n\nThe Grammar of Graphics consists of the following key components:\n\n- Data: The data you want to visualize.\n- Aesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc.\n- Geometries (geom): The type of plot, like points, lines, bars, etc.\n- Facets: Subplots based on the data.\n- Scales: Control the mapping from data to aesthetic properties.\n- Coordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.).\n- Themes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n**3. Creating Your First Plot**\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a basic scatter plot\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:3: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-5-1.png){width=614}\n:::\n:::\n\n\n\n\n\n** 4. Adding Aesthetic Mappings**\n\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\n\nExample: Color by cyl (number of cylinders)\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:1: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-6-3.png){width=614}\n:::\n:::\n\n\n\n\n\nExample: Size by horsepower (hp)\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:1: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-7-5.png){width=614}\n:::\n:::\n\n\n\n\n\n**5. Geometric Objects**\n\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\n\nExample: Adding a smooth line (geom_smooth)\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-8-7.png){width=614}\n:::\n:::\n\n\n\n\n\n**6. Faceting**\n\nFaceting allows you to split your plot into multiple panels based on a factor.\n\nExample: Facet by cyl\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom plotnine import facet_wrap\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-9-9.png){width=614}\n:::\n:::\n\n\n\n\n\n**7. Customizing Scales**\n\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\n\nExample: Custom Color Scale\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom plotnine import scale_color_manual\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-10-11.png){width=614}\n:::\n:::\n\n\n\n\n\n**8. Flip Coordinates**\nCreate a bar plot showing distribution of cylinders\n\nExample: Fliping coordinates axis\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nfrom plotnine import ggplot, aes, geom_bar, coord_flip, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a bar plot showing distribution of cylinders\n(ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:3: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-11-13.png){width=614}\n:::\n:::\n\n\n\n\n\n**9. Themes**\n\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\n\nExample: Apply a Minimal Theme\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom plotnine import theme_minimal\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\n<Figure Size: (640 x 480)>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Baruga_Main_files/figure-html/unnamed-chunk-12-15.png){width=614}\n:::\n:::\n\n\n\n\n\n**10. Saving the Plot**\n\nYou can save your plot using the save method.\n\nExample: Save the plot\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n**Week 5 Midterm Report**\n\n\n**Data Analysis**\n\n**Load libraries**\n\n\n```python\nimport torch as tch\nimport pandas as pd\nimport scipy as sci\nimport openpyxl as opxl\nimport seaborn as sns\n```\n\n**Load CSV**\n\n\n```python\ndf = pd.read_csv('data.csv', low_memory=False)\n```\n\n**Summary Stats**\n\n\n```python\ndf.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SCRMCTRL</th>\n      <th>PPCSWGT</th>\n      <th>SEQNUM</th>\n      <th>SEX</th>\n      <th>AGE</th>\n      <th>INTTYPE</th>\n      <th>NONINT</th>\n      <th>HISP</th>\n      <th>MODE</th>\n      <th>PSSTRATA</th>\n      <th>...</th>\n      <th>V352A</th>\n      <th>V352B</th>\n      <th>V352C</th>\n      <th>V352D</th>\n      <th>V352E</th>\n      <th>V352F</th>\n      <th>CHECK_ITEM_J</th>\n      <th>CHECK_ITEM_K</th>\n      <th>CHECK_ITEM_L</th>\n      <th>V353</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.030020e+18</td>\n      <td>4150.904836</td>\n      <td>1.0</td>\n      <td>(1) Male</td>\n      <td>(4) 45-64</td>\n      <td>(2) PPCS Interview - Telephone</td>\n      <td>NaN</td>\n      <td>(1) White Only</td>\n      <td>(0) Computer-assisted personal interviewing</td>\n      <td>54</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.030030e+18</td>\n      <td>1601.829088</td>\n      <td>2.0</td>\n      <td>(2) Female</td>\n      <td>(3) 25-44</td>\n      <td>(2) PPCS Interview - Telephone</td>\n      <td>NaN</td>\n      <td>(1) White Only</td>\n      <td>(0) Computer-assisted personal interviewing</td>\n      <td>8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.030030e+18</td>\n      <td>0.000000</td>\n      <td>3.0</td>\n      <td>(1) Male</td>\n      <td>(1) 16-17</td>\n      <td>(5) PPCS Noninterview</td>\n      <td>(5) NCVS Interview Completed by Proxy</td>\n      <td>(2) Black Only</td>\n      <td>(0) Computer-assisted personal interviewing</td>\n      <td>8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.030030e+18</td>\n      <td>0.000000</td>\n      <td>4.0</td>\n      <td>(1) Male</td>\n      <td>(3) 25-44</td>\n      <td>(5) PPCS Noninterview</td>\n      <td>(5) NCVS Interview Completed by Proxy</td>\n      <td>(1) White Only</td>\n      <td>(0) Computer-assisted personal interviewing</td>\n      <td>8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.030030e+18</td>\n      <td>1672.290183</td>\n      <td>5.0</td>\n      <td>(1) Male</td>\n      <td>(3) 25-44</td>\n      <td>(2) PPCS Interview - Telephone</td>\n      <td>NaN</td>\n      <td>(2) Black Only</td>\n      <td>(0) Computer-assisted personal interviewing</td>\n      <td>37</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 266 columns</p>\n</div>\n\n\n\n**TRace (HISP) and CHECK_ITEM_L**\n\nCHECK_ITEM_L is the survey question asked to participants on whether or not they have been arrested before\n\n\n```python\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP'])\n\n# Create a count plot\nplt.figure(figsize=(12, 6))\nsns.countplot(data=df_filtered, x='HISP', hue='CHECK_ITEM_L', palette='viridis')\n\n# Add titles and labels\nplt.title('Count of CHECK_ITEM_L by HISP', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels\nunique_labels = df_filtered['CHECK_ITEM_L'].dropna().unique()\nlabel_names = {1: 'Yes', 2: '2', 9: 'Out of universe'}\nplt.legend(title='CHECK_ITEM_L', labels=[label_names.get(label, str(label)) for label in unique_labels])\n\nplt.grid(axis='y')\nplt.show()\n```\n\n\n    \n![png](output_8_0.png)\n    \n\n\nThis plot, generated from the 2018 Police-Public Contact Survey, displays the count of interactions where respondents answered “Yes” to a specific question (CHECK_ITEM_L), categorized by race (HISP). The survey investigates civilian experiences with law enforcement. The highest count is observed among White individuals, which is consistent with their larger representation in the dataset. In contrast, racial groups such as Asians and others have significantly fewer “Yes” responses. This chart highlights racial differences in certain law enforcement-related interactions, suggesting possible disparities in how different groups experience or report these encounters.\n\n## Race arrests by SEX with SCRMCTRL as Count Variable\n\n\n```python\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# SEX Mapping\nsex_mapping = {\n    '(1) Male': 1,   # Male\n    '(2) Female': 2, # Female\n}\n\ndf['SEX'] = df['SEX'].astype(str).map(sex_mapping)\ndf['SEX'] = pd.to_numeric(df['SEX'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP', 'SEX', 'SCRMCTRL'])\n\n# Create a count plot with SCRMCTRL as the count variable\nplt.figure(figsize=(12, 6))\n\n# Count the occurrences of SCRMCTRL and use it to plot\nsns.countplot(data=df_filtered, x='HISP', hue='SEX', palette='viridis', dodge=True)\n\n# Add titles and labels\nplt.title('Count of Race arrests by SEX with SCRMCTRL as Count Variable', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count of SCRMCTRL', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels for SEX\nplt.legend(title='SEX', labels=['Male', 'Female'])\n\nplt.grid(axis='y')\n\n# Save the plot as a PNG file\nplt.savefig('arrests_histogram_baruga.png', format='png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n```\n\n\n    \n![png](output_11_0.png)\n    \n\n\nThis plot, based on the 2018 Police-Public Contact Survey dataset, shows the count of arrests (SCRMCTRL) across different racial categories (HISP), segmented by gender (SEX). The survey, conducted by the U.S. Bureau of Justice Statistics, examines public interactions with law enforcement, such as police stops and arrests. The higher arrest count for White males can be attributed to the fact that White individuals make up the largest racial group in the dataset. The data reveals notable racial and gender disparities in arrests, with arrest counts for females across all racial categories being lower than for males.\n\n",
    "supporting": [
      "Baruga_Main_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}