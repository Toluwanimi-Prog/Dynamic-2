[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dynamic Group 2",
    "section": "",
    "text": "Preface\nHello and Welcome!\nThis book is dedicated to showcasing the work of Group 2, a dynamic team of dedicated students from the Visualization class at the prestigious Oral Roberts University, Oklahoma, under the guidance of Dr. V. Here, you’ll find weekly Quarto documents and exciting projects as they explore the world of data visualization.\nThe Members of the Group 2 are\n\nAbigail Tako\nCaleb Pena\nDerrick Baruga\nToluwanimi Olufawo\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Welcome to Group 2’s Book\nThis book is dedicated to showcasing the work of Group 2, a dynamic team of dedicated students from the Visualization class at the prestigious Oral Roberts University, Oklahoma, under the guidance of Dr. V. Here, you’ll find weekly Quarto documents and exciting projects as they explore the world of data visualization.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Introduction.html#group-members",
    "href": "Introduction.html#group-members",
    "title": "1  Introduction",
    "section": "2.1 Group Members",
    "text": "2.1 Group Members\n\nAbigail Tako\nCaleb Pena\nDerrick Baruga\nToluwanimi Olufawo",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html",
    "href": "Tako_Main.html",
    "title": "2  Abigail Tako",
    "section": "",
    "text": "2.1 Monday\n1st Dashboard - Economics_ggplot2\nClick on this link to see and interact with the dashboard! https://public.tableau.com/views/AbigailTako_Econoomics_dashboard/Dashboard1?:language=en-\nVariables\nDescription\nFigure 1.1\nBox plots showing the distribution of the four variables (Pce, Pop, Uemmped, Unemploy) across multiple years. This gives insight into the central tendency, spread, and outliers for each variable.\nFigure 1.2\nA bar chart showing the unemployment count for each year, with different colors representing different years, emphasizing trends in unemployment across time.\nFigure 1.3\nStacked charts showing how Pce, Unemploy, Uempmed, and Pop have changed over the years. This provides a timeline for comparing how these variables behave in relation to one another, revealing patterns of economic cycles.\n2nd Dashboard - Individual Project\nClick on this link to see and interact with the dashboard! https://public.tableau.com/views/IndividualProjectTableau_17262585364150/Dashboard1?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link\nVariables\nDescription\nFigure 1.1\nThis line chart displays the overall trends in key traffic-related variables over the years. Crashes (green line) have shown a slight decline but remain relatively stable. Fatalities (red line) have stayed consistent, fluctuating around 30,000 to 40,000 per year. Injured persons (blue line) exhibit a gradual downward trend, with a noticeable dip in 2015, possibly due to reporting changes or other factors. Vehicle-miles traveled (brown line) show a sharp drop in 2015 but are generally consistent, indicating steady road usage over the years. The chart provides a high-level overview of the interactions between these variables over time.\nFigure 1.2\nIt shows a decreasing trend in injuries until around 2015, after which it fluctuates. Fatalities also display minor fluctuations but generally remain around the 30,000 to 40,000 range per year.\nFigure 1.3\nThe boxes represent the interquartile range, while the whiskers capture the spread of data, offering a clear view of central tendencies (mean) and variability across the years 2000 to 2023. This chart highlights the typical values and outliers for crashes, fatalities, injuries, and miles traveled.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#wednesday",
    "href": "Tako_Main.html#wednesday",
    "title": "2  Abigail Tako",
    "section": "2.2 Wednesday",
    "text": "2.2 Wednesday\n\n2.2.1 Week 1\n\n\n2.2.2 Week 2\nHistogram\nIn week 2, data sets that I am using is air quality data set. After cleaning the data set explained below, select all the data and insert the histogram chart to create the visualization of the data. The variables for the histogram chart is only the ozone. From the histogram, it shows the distribution of the ozone, most of the ozone are 1 to 25 ppm.\n\nclean data\nFirst, from the data, we are going to clean the data, by removing those that are have the value NA or not available.\n1. Select all the data, and search for the filter.\n2. Then, it will appear the drop down menu for each column, select the drop down menu on the first column which is Ozone\n3. Select NA. To remove it, select all from the row that has NA until the bottom and select delete rows 6-151.\n4. Select the drop down menu on column Ozone, select all and apply, it will give back the table, however there’s no NA anymore on the Ozone.\nLooking at the data, there still some NA on the column B for Solar.R. Repeat the way just like before. Next, to make it earlier to read the data, sort the day and month to be in order. Select the column for month, click the sort from smallest to largest, do the same for day.\nScatter plot\nUsing the air quality data sets, click on column A(ozone) and D (temperature) to show the correlation between both of them. After clicking on both column, I’m going to insert the scatter plot. The scatter plot shows a relationship between Ozone levels (y-axis) and Temperature (x-axis). The general trend indicates a positive correlation: as the temperature increases, the ozone level tends to increase as well.\n\nPivot Table & Chart\nTo make a pivot table and chart, first select all the data, and click insert. On the let corner, it will appear the pivot menu, press that and select the create own pivot table. I select and drag ozone, temperature, Solar. R, and Wind to the value, for the row I’ll put only the month. Then, I change the value field settings to average, this is to provide the average of ozone, temperature, Solar. R, and Wind monthly. After doing the pivot table, select that pivot table and insert the pivot chart to create the visualization.\n\nFrom the pivot table and chart above, it shows that ozone levels appear to increase with higher temperature and solar radiation but decrease with higher wind speeds.\nNext, I’m going to provide another pivot table, that shows the average of ozone, temperature, Solar. R, and Wind daily from day 1 to 31. To do that, select again the first table, the one that was cleaned, then again select the pivot table and choose the create own pivot table.\nI select and drag ozone, temperature, Solar. R, and Wind to the value, for the row I’ll put only the day. Then, I change the value field settings to average, this is to provide the average of ozone, temperature, Solar. R, and Wind per day.\nNow, I’m going to add the pivot chart from that new pivot table. I choose two charts, from those two I can create conclusion and better visualizations.\nFrom the chart presented below, the average daily of Temperature and Wind each have a positive correlation, however not with the average daily of Solar. R and the Ozone. Also, on day 15 it shows the lowest average of ozone and solar. r.\n\nThird, I’m going to make another pivot table that can show more detailed on each month per day.I’m going to choose to present the 5th month, so I’m going to press the drop down menu from the column “month” and select only the 5. Next, the table will only provide the information from month 5, day 1-31 and the sum of ozone, wind, temp, and solar.r.\n\nFrom the chart, we can see that, on day 30 from the 5th month, it shows the highest sum of ozone.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#friday---midterm-projects",
    "href": "Tako_Main.html#friday---midterm-projects",
    "title": "2  Abigail Tako",
    "section": "2.3 Friday - Midterm Projects",
    "text": "2.3 Friday - Midterm Projects\n\n2.3.1 Week 1\n\n\n2.3.2 Week 2\nWho collected the data\nThe source that I chose is National Transportation Library (NTL) Data. There are lots of data sets from NTL, which can be access through https://ntl.bts.gov/ntl. Data set that I chose is motor vehicle safety. This data set is collected by U.S. Department of Transportation, National Highway Traffic Safety Administration, National Center for Statistics and Analysis, and Fatality Analysis Reporting System (FARS) Database. I’m interested in this one, because it is critical to understand trends in road safety, which is a major public concern. The data that I collect is from this link https://www.bts.gov/content/motor-vehicle-safety-data.\nPurpose\nMonitoring the trends in motor vehicle safety in the U.S., including fatalities, injuries, and crashes. It serves to assess the effectiveness of safety regulations, technology advancements, and policy interventions over time. From the visualizations, it can improve the safety measures and highlight the effectiveness of road safety interventions over the years, making it highly valuable for assessing long-term changes in traffic-related fatalities and injuries.\nVariables\n\nFatalities - total number of deaths from motor vehicle crashes\nInjured persons - total number of people injured in motor vehicle accidents\nCrashes - total number of motor vehicle crashes\nVehicles miles traveled - total number of miles driven by vehicles\n\n\n\n2.3.3 Week 5\nMid Term Project\n\nVariables in the Graph:\n\nTime (X-axis)\nYears from 1980 to 2022\nGDP (Y-axis)\nU.S. Gross Domestic Product (GDP) attributed to for-hire transportation services, measured in billions of current dollars.\n\nCategories of Transportation Services (Stacked Bars)\n\nAir transportation\nTruck transportation\nRail transportation\nWater transportation\nPipeline transportation\nTransit and ground passenger transportation\nOther transportation and support activities\nWarehousing and storage\n\nKey Insights\nThe graph illustrates the consistent growth of the U.S. GDP from for-hire transportation services between 1980 and 2022. Truck and air transportation sectors contribute the largest shares, with noticeable growth across all categories post-2010. The overall upward trend reflects the increasing economic importance of transportation services in the U.S.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#jupyter-notebooks",
    "href": "Tako_Main.html#jupyter-notebooks",
    "title": "2  Abigail Tako",
    "section": "2.4 Jupyter Notebooks",
    "text": "2.4 Jupyter Notebooks\n\n2.4.1 Week 4\nThis is a markdown title\nin markdown we can create lists:\n\nitem 1\nitem 2\nitem 3\n\nalso we can create enumerated list\n\nHola\nHi\nNamaste\n\nwe can do bold, also italic\n# List are native to Python\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1\n[1 2 3 4 5]\n# List are native to Python\nmy_list = [1, 2, 3, 4, 5]\nprint (my_list)\n[1, 2, 3, 4, 5]\n# We will be using a lot of data frames, so we need pandas library\n# Panda allows us to make small spreadsheets, there will be rows and columns\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n4. Loading CSV files To load files into a DataFrame , we use the pandas function read_csv;\n# Load and analyze data from a CSV file named airquality_datasets.csv using the pandas library\ndf = pd.read_csv('airquality_datasets.csv')\n# df.info () -&gt; displays the structure and non-null counts of the DataFrame.\n# df.describe () -&gt; provides summary statistics such as mean, standard deviation, minimum, and maximum values for each column.\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\nInstant data view - JupyterLab allows us to instantly view the structure and data types of the columns within the DataFrame by using df.info(). This displays a concise summary of each column, including the number of non-null entries and the type of data (e.g., float64, int64). - This feature makes it easy to verify that the data types (like floats for Temp, Ozone, Wind, etc.) align with the expected values from the CSV file, ensuring the data is in the correct format for further analysis and visualization.\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\nVisual Insights - We can quickly identify trends and the spread of values. - Peaks in the histograms indicate where data points are densely clustered, revealing common or typical readings in the dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#week-4",
    "href": "Tako_Main.html#week-4",
    "title": "2  Abigail Tako",
    "section": "3.2 Week 4",
    "text": "3.2 Week 4\n\nAbigail’s Notebook: This notebook contains the data cleaning and exploration tasks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#this-is-a-markdown-title",
    "href": "Tako_Main.html#this-is-a-markdown-title",
    "title": "2  Abigail Tako",
    "section": "2.5 This is a markdown title",
    "text": "2.5 This is a markdown title\nin markdown we can create lists:\n\nitem 1\nitem 2\nitem 3\n\nalso we can create enumerated list\n\nHola\nHi\nNamaste\n\nwe can do bold, also italic\n# List are native to Python\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1\n[1 2 3 4 5]\n# List are native to Python\nmy_list = [1, 2, 3, 4, 5]\nprint (my_list)\n[1, 2, 3, 4, 5]\n# We will be using a lot of data frames, so we need pandas library\n# Panda allows us to make small spreadsheets, there will be rows and columns\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#loading-csv-files",
    "href": "Tako_Main.html#loading-csv-files",
    "title": "2  Abigail Tako",
    "section": "2.6 4. Loading CSV files",
    "text": "2.6 4. Loading CSV files\nTo load files into a DataFrame , we use the pandas function read_csv;\n# Load and analyze data from a CSV file named airquality_datasets.csv using the pandas library\ndf = pd.read_csv('airquality_datasets.csv')\n# df.info () -&gt; displays the structure and non-null counts of the DataFrame.\n# df.describe () -&gt; provides summary statistics such as mean, standard deviation, minimum, and maximum values for each column.\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#instant-data-view",
    "href": "Tako_Main.html#instant-data-view",
    "title": "2  Abigail Tako",
    "section": "2.7 Instant data view",
    "text": "2.7 Instant data view\n\nJupyterLab allows us to instantly view the structure and data types of the columns within the DataFrame by using df.info(). This displays a concise summary of each column, including the number of non-null entries and the type of data (e.g., float64, int64).\nThis feature makes it easy to verify that the data types (like floats for Temp, Ozone, Wind, etc.) align with the expected values from the CSV file, ensuring the data is in the correct format for further analysis and visualization.\n\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#visual-insights",
    "href": "Tako_Main.html#visual-insights",
    "title": "2  Abigail Tako",
    "section": "2.8 Visual Insights",
    "text": "2.8 Visual Insights\n\nWe can quickly identify trends and the spread of values.\nPeaks in the histograms indicate where data points are densely clustered, revealing common or typical readings in the dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html",
    "href": "Caleb_Main.html",
    "title": "3  Caleb Pena",
    "section": "",
    "text": "3.1 Wednesday",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#wednesday",
    "href": "Caleb_Main.html#wednesday",
    "title": "3  Caleb Pena",
    "section": "",
    "text": "3.1.1 Week 1\n\n\n3.1.2 Week 2\nWednesday, September 4, 2024\nI am using the airquality dataset, which is a collection of data collected from 153 observations based on 6 different variables. These variables include:\nOzone (Numeric) - Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island\nSolar R (Numeric) - Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park\nWind (Numeric) - Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport\nTemp (Numeric) - Maximum daily temperature in degrees Fahrenheit at La Guardia Airport\nMonth (Numeric) - Months from May to September\nDay (Numeric) - Days of the months ranging from 1 to 31\nI am using Excel to clean the data, which removes all the rows which have N/A values, and I did an exploration analysis.\nOzone vs Temperature Histogram\nThis histogram represents the amount of temperature values calculated within each border of values of ozone. For example, in the range of ozone level between 1 and 25, there were 49 temperature values recorded.\n\nOzone vs Temperature Scatter Plot\nThis scatter plot displays the correlation between the ozone levels and the temperature values recorded within these five months. As the ozone level increased, so did the temperature value, with one outlier.\n\nMy First PivotTable\nThis PivotTable displays the average values of each variable for each of the five months.\n\nMy Second PivotTable\nThis PivotTable displays the average temperature values for each day of each month.\n\n\n\n3.1.3 Week 3\nWednesday, September 11\nThe data from the Cars dataset was recorded in the 1920s and focuses on two different variables:\nSpeed (Numerical) - The speed in mph at which the car was traveling Stop Distance (Numerical) - The distance in ft that it took for the car to stop while traveling a specific speed\nThis is the dashboard: \nThe graph “Speed/DistAgainstSpeed+DistDualAxis” was a dual axis plot that measured the variables speed and stop distance against a new calculated variable called speed+dist. This graph simply shows how the distance values have a much greater numerical value than the speed values.\nThe graph “SpeedAndDistBoxPlot” was a box plot that measured the average distance per each speed value and calculated all the data into one box plot. This plot shows the different values for average stopping distance in comparison to all the speed values.\nThe graph “AverageSpeedPerDistance” is a line plot that calculates the average speed per each distance value. There is a correlation between the values of average speed and distance; as distance values increase, the average speed of which the car was traveling also increases.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#friday---midterm-projects",
    "href": "Caleb_Main.html#friday---midterm-projects",
    "title": "3  Caleb Pena",
    "section": "3.2 Friday - Midterm Projects",
    "text": "3.2 Friday - Midterm Projects\n\n3.2.1 Week 1\n\n\n3.2.2 Week 2\nFriday, September 6, 2024\nUsing the same airquality datasets, I worked on creating PivotCharts from previous and new PivotTables comparing different values and variables.\nPivotChart1 - Average Variable Values by ‘Month’\nThis pivot table produced this chart. These are the average values of each variables for each of the five months. The PivotChart shows the comparison between each of the variables and how each one increases or decreases throughout the five months.\n\n\nPivotChart2 - Average Ozone Values by ‘Temp’\nThis pivot table produced this chart. This PivotChart displays the average ozone values for each temperature value that was recorded. As the temperature values increase, the average values for the ozone can be seen to also increase in a correlated way.\n\n\nPivotChart3 - Average Temp and Ozone Values by ‘Wind’\nThis pivot table produced this chart. This PivotChart displays the average values of both temperature and ozone for each wind value recorded. From the chart, it is evident that as the wind values increase, the average values for temperature seem to slightly decrease and the average values for the ozone levels seem to greatly decrease.\n\n\nNewport Oregon Oceanographic Temperature Dataset\nStarting off in the year 1996, a group of NOAA Fisheries and Oregon State University Scientists sampled the Newport Hydrographic Line fortnightly to understand changing ocean conditions. The various variable data was collected from a station located 5 miles off the coast of Newport, Oregon. The scientists sampled and collected data regarding seven different variables:\nDay (Numerical) - The specific day of the month in which the data was sampled.\nMonth (Numerical) - The specific month of the year in which the data was sampled.\nYear (Numerical) - The specific year in which the data was sampled.\nTemperature (Numerical) - Oceanographic, temperature data collected from a 50 m water depth.\nOxygen (Numerical) - Oceanographic, oxygen data collected from a 50 m water depth.\nNorthern Copepod Biomass (Numerical) - Copepod community data collected from from vertical net samples on the northern area\nSouthern Copepod Biomass (Numerical) - Copepod community data collected from from vertical net samples on the southern area\nVisualizations\nThe descriptive statistics of all the water temperatures sampled throughout the 28 years. The average temperature of all collective 28 years is 8.75 ºC. The lowest temperature recorded of all 28 years is 6.92 ºC and the highest temperature recorded from all 28 years is 14.65 ºC.\n\nThe peak temperatures of the water during the winter have a higher average than the peak temperatures of the water during the summer.\n\nThrough the dot plot, the average of the temperatures is 8.75 ºC as most of the dot population is visible there.\n\nThe average temperatures of all collective 28 years gathered within each month. During the summer months, the averages are lower. During the winter months, the averages are higher.\n ### Week 3\n###Week 3\nFriday, September 13\nThe Oceanographic data from the Newport Line along Oregon was gathered from 1996 - present and focuses on seven different variables.\nSample Date (Date) - The exact date of which the rest of the variables were sampled\nMonth (Categorical) - The month of which the variables were sampled\nYear (Numerical) - The year of which the variables were sampled\nAverage Of Temperature (Numerical) - The average oceanographic temperature sampled 50 meters deep during the date it was sampled\nAverage of Oxygen (Numerical) - The average oceanographic temperature sampled 50 meters deep during the date it was sampled\nNorthern Copepod Biomass (Numerical) - The units of carbon biomass per cubic meter of copepods in the northern section of the sample site\nSouthern Copepod Biomass (Numerical) - The units of carbon biomass per cubic meter of copepods in the southern section of the sample site\nThis is the dashboard: \nThe graph “AverageOceanographicTemperatureEachMonth” was a box plot that measured the variables Avg of Temperature against the categorical variable month. Each value of temperature that was sampled on its sample date was recorded for each month. This graph shows how during the summer months, the oceanographic temperature was lower than during the winter months, differing from what would be commonly assumed.\nThe graph “AverageOceanographic(Temp/Oxygen)EachYear” was a dual axis plot that measured the variables Avg of Temperature and Avg of Oxygen against the categorical variable year. This plot shows the average values of these two variables for each month so the viewer can see how these values have changed throughout the years.\nThe graph “EffectsOf(Temp/Oxygen)On(Northern/Southern)CopepodBiomass” is a dual axis plot that measures four different variables. This plot focuses on the effects that the oceanographic temperature and oceanographic oxygen have on the population and copepod biomass recorded on the sample dates and near the sample areas. From the graph, it is visible that as oceanographic oxygen rises, the northern copepod biomass greatly decreases and the southern copepod biomass slightly increases. It is also evident that as oceanographic temperature rises, the northern copepod biomass also greatly decreases and the southern copepod biomass also slightly increases.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#jupyter-notebooks",
    "href": "Caleb_Main.html#jupyter-notebooks",
    "title": "3  Caleb Pena",
    "section": "3.3 Jupyter Notebooks",
    "text": "3.3 Jupyter Notebooks\n\n3.3.1 Week 4\n\nCaleb’s Notebook: This notebook contains the data cleaning and exploration tasks.\n\nThis is a markdown title\nIn markdown, we can create lists:\n\nItem 1\nItem 2\nItem 3\n\nWe can also create enumerated lists:\n\nHola\nHi\nNamaste\n\nWe can bold and italic\n# Here we are importing numby with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1\n[1 2 3 4 5]\n# Lists are native to Python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# We will be using a lot of dataframes, so we need Pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n4. Loading csv files\nTo load .csv files into a DataFrame, we use the Pandas function read.csv:\ndf = pd.read_csv('airquality_datasets.csv')\nprint(df.info())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\nprint(df.describe())\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\n5. Visualizing the Dataset\nLet’s dive into visualizations using matplotlib. We’ll start with simple histograms and boxplots, then move on to correlation plots.\nHistograms Histograms help us understand the distribution of the variables. We’ll create histograms for Ozone and Temp.\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\nMatplotlib is building the font cache; this may take a moment.\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#week-4",
    "href": "Caleb_Main.html#week-4",
    "title": "3  Caleb Pena",
    "section": "3.4 Week 4",
    "text": "3.4 Week 4\n\nCaleb’s Notebook: This notebook contains the data cleaning and exploration tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#this-is-a-markdown-title",
    "href": "Caleb_Main.html#this-is-a-markdown-title",
    "title": "3  Caleb Pena",
    "section": "3.4 This is a markdown title",
    "text": "3.4 This is a markdown title\nIn markdown, we can create lists:\n\nItem 1\nItem 2\nItem 3\n\nWe can also create enumerated lists:\n\nHola\nHi\nNamaste\n\nWe can bold and italic\n# Here we are importing numby with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1\n[1 2 3 4 5]\n# Lists are native to Python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# We will be using a lot of dataframes, so we need Pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#loading-csv-files",
    "href": "Caleb_Main.html#loading-csv-files",
    "title": "3  Caleb Pena",
    "section": "3.5 4. Loading csv files",
    "text": "3.5 4. Loading csv files\nTo load .csv files into a DataFrame, we use the Pandas function read.csv:\ndf = pd.read_csv('airquality_datasets.csv')\nprint(df.info())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\nprint(df.describe())\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Caleb_Main.html#visualizing-the-dataset",
    "href": "Caleb_Main.html#visualizing-the-dataset",
    "title": "3  Caleb Pena",
    "section": "3.6 5. Visualizing the Dataset",
    "text": "3.6 5. Visualizing the Dataset\nLet’s dive into visualizations using matplotlib. We’ll start with simple histograms and boxplots, then move on to correlation plots.\n\n3.6.1 Histograms\nHistograms help us understand the distribution of the variables. We’ll create histograms for Ozone and Temp.\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\nMatplotlib is building the font cache; this may take a moment.\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Caleb Pena</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html",
    "href": "Baruga_Main.html",
    "title": "4  Derrick Baruga",
    "section": "",
    "text": "4.1 Wednesday",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#wednesday",
    "href": "Baruga_Main.html#wednesday",
    "title": "4  Derrick Baruga",
    "section": "",
    "text": "4.1.1 Week 1\n\n\n4.1.2 Week 2\nI am using the air-quality dataset, which contains air quality measurements collected over several months, specifically from May to September. The dataset includes the following variables: Ozone, Solar.R (solar radiation), Wind (wind speed in miles per hour), Temp (temperature in degrees Fahrenheit), Month (ranging from 5 for May to 9 for September), and Day (ranging from 1 to 31).\nCleaning The Data\nI used Excel to clean the data by removing all rows with NA values and performed an exploratory analysis to identify patterns, trends, and potential relationships between these variables over the specified months and days.\nOzone Histogram\n\nThe Ozone histogram has its highest values on the left and then curbs off to the right, it indicates a right-skewed distribution.\nThis means:\n\nMost ozone values are low, with a few much higher values creating a long tail on the right.\nThe skewness could result from natural variability, pollution events, or weather conditions affecting ozone levels.\n\nScatter Plot Ozone vs Temperature\n\n\nA scatter plot of ozone (x-axis) vs. temperature (y-axis) with a slight positive correlation means that higher ozone levels tend to be associated with higher temperatures. However, the relationship is weak, suggesting other factors (like wind, humidity, or pollution) also affect ozone levels and temperature.\n\nPivot Table Average Ozones Per Day & Month (Redacted)\n\n\nThe pivot table presents daily sums for multiple variables (Temperature, Wind, Solar Radiation, and Ozone) over several months (May to September). The days with the highest ozone averages are Day 25 in May (with an Ozone value of 5-65) and July (with an Ozone value of 7-74), with other notable days being Day 29 and Day 30 in May. The months with the highest ozone levels are May, which shows several days with high averages, and July, particularly on Days 25 and 29.\n\nPivot Chart Average Ozones Per Day & Month\n\nPivot Table of Variation in Solar Radiation and Temperature\n\nThe pivot chart provide a visual representation of how these variables change over time. The histogram shows day-to-day variations in ozone levels, with the highest concentrations occurring in the summer months. Notably, there is a pronounced peak around July 25th, indicating exceptionally high ozone levels on that day. May and July both have elevated levels, with smaller peaks around May 29th and 30th, but July 25th stands out as the most significant. Overall, the chart confirms that ozone levels are highest in July, particularly around the 25th.\n\n\n\nThe pivot table shows daily sums for Solar Radiation and Temperature over a month. There is noticeable daily variation, with high values on days like 18, 19, and 29, indicating intense sunlight and warmer temperatures, and lower values on days like 23 and 27, reflecting cooler conditions. The grand totals summarize the entire month, with 20,513 for Solar Radiation and 8,635 for Temperature. Overall, the table captures daily fluctuations in weather conditions.\n\n** Pivot Chart of Pivot Chart of Variation in Solar Radiation and Temperature**\n\n\nThe bar chart shows daily sums of Solar Radiation (blue) and Temperature (orange) over 31 days. High Solar Radiation is notable on days like 9, 13, 16, 18, 19, and 29, with values exceeding 1,000. Temperature values are generally lower, mostly below 400, with higher values on days like 9 and 18. There are significant day-to-day variations, with some days showing high Solar Radiation but lower temperatures (e.g., Day 13). The chart captures daily fluctuations and highlights days with extreme values.\n\nPivot Table\n\n\nThe pivot table displays the average temperature (Average of Temp) and ozone levels (Average of Ozone) for days labeled 5 to 9, showing average temperatures ranging from 66.46 to 83.88, with the highest temperatures recorded on days 7 and 8. Ozone levels vary significantly, from a low of 24.13 on Day 5 to a high of 60.00 on Day 8. Overall, the average temperature for the period is 77.79, and the average ozone level is 42.10, reflecting moderate temperatures with variable ozone levels across these days and highlighting daily fluctuations in both metrics.\n\nPivot Chart\n\n\nThe bar chart illustrates the average temperature (in blue) and average ozone levels (in orange) for days 5 to 9, showing that temperatures remain relatively high throughout, ranging from around 66 on Day 5 to approximately 84 on Days 7 and 8. Ozone levels start low on Day 5 (around 24), rise significantly by Days 7 and 8 (around 59-60), and then decrease slightly on Day 9 (around 31). The chart suggests a potential correlation between higher temperatures and elevated ozone levels, as Days 7 and 8, which have the highest temperatures, also show the highest average ozone levels, indicating noticeable variability over the period.\n\n\n\n4.1.3 Week 3\nWednesday Dashboard\n\nHistogram of Avg REM Sleep:\n\n\nThis bar chart shows the average REM sleep time categorized by both conservation status (like “cd,” “en,” “lc”) and dietary habits (carnivore, herbivore, omnivore, etc.).\nThe distribution indicates variations in REM sleep depending on these factors. For example, certain categories, such as carnivores with an “lc” (least concern) conservation status, seem to have higher average REM sleep.\n\n\nStacked Bar Chart of Vore vs. REM Sleep:\n\n\nThis chart displays the breakdown of average REM sleep across different dietary categories and their conservation statuses, further subdivided by animal order.\nIt provides detailed insights into how sleep patterns differ based on dietary habits and animal groups. For instance, “Carnivora” under different conservation statuses like “lc” and “domesticated” shows varying REM sleep levels.\n\n\nPacked Bubble Chart of Order:\n\n\nThe chart uses bubbles to represent different orders of animals, with the size of each bubble possibly corresponding to the number of species or the average REM sleep within that order.\nColors differentiate various dietary habits (“vore”), showing how different animal orders fall into categories such as carnivores, herbivores, omnivores, etc.\n\n\nTreemap of Order:\n\n\nThis chart breaks down the animal orders into smaller rectangles, where the size of each rectangle may indicate the average REM sleep, the number of species, or another quantitative measure.\nThe colors correspond to different orders, offering a visual overview of how these orders compare in the measured metric.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#friday---midterm-projects",
    "href": "Baruga_Main.html#friday---midterm-projects",
    "title": "4  Derrick Baruga",
    "section": "4.2 Friday - Midterm Projects",
    "text": "4.2 Friday - Midterm Projects\n\n4.2.1 Week 1\n\n\n4.2.2 Week 2\ntitle: “Midterm_Project_Baruga” subtitle: “Data.gov: Warehouse and Retail Sales” format: html —\nContext of the Dataset\n\nTitle: Warehouse and Retail Sales\nLink: Download the CSV dataset\n\nThe Warehouse and Retail Sales dataset provides a comprehensive view of sales activities in Montgomery County, Maryland, by capturing data from various warehouse and retail operations. This dataset was collected through a combination of direct reporting from businesses, automated sales tracking systems, and regional sales surveys. The data encompasses a range of sales metrics, including volume and product categories, to offer insights into business performance across different types of establishments.\nThe dataset includes the following variables:\n\nRow Labels: Categories or identifiers used to organize and classify the sales data.\nBEER: Sales volume for beer products.\nDUNNAGE: Sales volume for dunnage products, which are materials used to protect goods during transportation.\nKEGS: Sales volume for kegs, typically used for storing and transporting beverages.\nLIQUOR: Sales volume for liquor products.\nNON-ALCOHOL: Sales volume for non-alcoholic beverages.\nREF: Sales volume for refrigeration supplies or products.\nSTR_SUPPLIES: Sales volume for store supplies, which may include various retail essentials.\nWINE: Sales volume for wine products.\nGrand Total: The total sales volume across all categories combined.\n\n** Data Cleaning and Preparation**\n\nImport the Data: I downloaded and loaded the CSV file into Excel.\nCheck for Missing Values: My preferred method for handling NAs is by highlighting them and deselecting them using the filter tool for each column as I feel most thorough by doing that. Using the following steps “Select the entire dataset, go to Home &gt; Conditional Formatting &gt; Highlight Cells Rules &gt; Blanks to highlight all blank cells.”\nData Formatting: Formated any columns that need specific data types (such as dates as date format, numbers as currency or percentage).\n\n\nSpecifically I used the =TEXTJOIN(“-”, TRUE, A2, B2) function to join column one (YYYY) and column two (MM) into a new column called TIME\n\n\nVisualization\nPivot Tables YT link: https://www.youtube.com/watch?v=qu-AK0Hv0b4\n\nPivot tables are a powerful and user friendly (drag and drop) summary statistic/visualisation tool that I used to get an intial feel of my data.\n\nHere are the findings:\nWarehouse Sales/Expenses of Beverages Over Time (2017 - 2020)\n\n\nOverall Sales: BEER has the highest sales, followed by WINE, while DUNNAGE, REF, and STR_SUPPLIES have negative or minimal sales, which makes sense as refers to materials used to protect goods during shipping and handling, such as packing materials or cushioning, and as such is an expense to the business leading to its negative output on revenue.\nMonthly Trends: BEER sales vary significantly, with large peaks and drops. WINE sales are more stable but still show some fluctuation. KEGS and LIQUOR show positive but lower sales.\n\nHistogram of Warehouse Sales/Expenses of Beverages Over Time\n\n\nHere is a histogram representation of the aforementioned summary pivot tables. There appears to be a disporportionate amount of BEER bought for warehouses. But that could be due to the fact that beers are sold in packs and so single unit quantity has skyrocketed in order to make a “12 pack” that will later count as one unit sold at retail.\n\nRetail Sales of Beverages Over Time (2017 - 2020)\n\n\nProduct Types: Includes Beer, Liquor, Wine, Non-Alcoholic beverages, and others, with substantial sales figures for Beer, Liquor, and Wine.\nSales Trends: Liquor leads with $802,691.43 in total sales, followed by Wine ($746,498.59) and Beer ($574,220.53). Recent months show higher sales for Beer and Wine.\nLow Sales Categories: Items like Dunnage and Kegs have negligible or zero sales.\nOverall Total: Total sales across all products amount to $2,160,899.37, highlighting overall retail activity.\n\nHistogram of Retail Sales of Beverages Over Time\n\n\nOnce we come to the retail side of things we see that WINE and LIQUOR are clear best sellers. It is shown hower that LIQUOR has begun to overtake WINE in retail sales.\n\nSummary\nFrom the “Warehouse and Retail Sales” dataset, I found that Beer leads in warehouse sales, with notable fluctuations due to bulk packaging, while Wine and Liquor have more stable sales. Retail sales show Liquor as the top seller, recently surpassing Wine, with Beer also performing strongly but declining. The histograms illustrate high Beer volume in warehouses and a shift in retail dominance from Wine to Liquor. Next, I will analyze seasonal trends, create advanced visualizations for deeper insights, and finalize the report with comprehensive findings and recommendations. ### Week 2 ### Week 3\n\n\n4.2.3 Week 3\nWednesday Dashboard\n1.Histogram of Avg Sales Over Time:\n\nThis stacked bar chart displays the average retail sales from June 2017 to September 2020, broken down by different item types such as “BEER,” “WINE,” “LIQUOR,” “NON-ALCOHOL,” and others.\nThe chart shows monthly fluctuations in sales, with noticeable peaks around December 2017 and July 2020. This suggests seasonal effects or particular periods of high demand for certain items.\nThe different colors represent various item types, indicating the contribution of each category to the total sales in each month. For example, “LIQUOR” and “NON-ALCOHOL” seem to contribute significantly to the total sales during peak months.\n\n2. Area Chart: Avg Sales vs. Transfers:\n\nThis plot consists of two layered area charts: the top one shows average retail transfers, and the bottom one shows average retail sales over the same period (June 2017 to September 2020).\nBoth charts use colors to represent different item types, revealing how each type contributes to overall sales and transfers.\nThe charts indicate that the trends in transfers often align with the sales trends, which suggests a correlation between the quantity of goods transferred and the sales performance.\nPeaks and troughs in the charts could indicate seasonal variations, inventory management strategies, or market demand shifts for various items.\n\n3. Packed Bubble Chart of Avg Sales Over Time:\n\nThis chart visualizes average sales using bubbles, where the size of each bubble reflects the volume of sales, and the color represents different item types.\nLarger bubbles correspond to higher sales, indicating which item types have the greatest impact on sales over time.\nThe variety of bubble sizes and colors reveals the diversity in item types and their varying sales performance.\n\n4. Treemap of Retail Transfers vs. Retail Sales:\n\nThe treemap displays retail sales data, with each rectangle representing different categories (“NON-ALCOHOL,” “BEER,” “LIQUOR,” “REF,” etc.) and specific years (2017, 2019, 2020).\nThe size of each rectangle corresponds to the magnitude of sales, and the color shading indicates the average retail sales, with darker shades representing higher sales.\nThis visualization shows how different item types and their sales vary in significance. For example, “NON-ALCOHOL” items appear to have a prominent share, especially in 2020.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#jupyter-notebooks",
    "href": "Baruga_Main.html#jupyter-notebooks",
    "title": "4  Derrick Baruga",
    "section": "4.3 Jupyter Notebooks",
    "text": "4.3 Jupyter Notebooks\n\n4.3.1 Week 4\nMarkdown title\nMarkdown Lists:\n\nItem 1\nItem 2\nItem 3\n\nEnumarated list\n\nHola\nHi\nNamaste\n\nWe can do bold, or italic\n# Importing Numpy with nickname np\nimport numpy as np\nnp.absolute(-1)\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n[1 2 3 4 5]\n# Lists are native to python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# Dataframes, so we need pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n4. Loading csv files\nTo load .csv files into a ‘DataFrame’, we use pandas function read_csv\ndf = pd.read_csv('/Users/derrickmarkbavaudbaruga/Documents/fall 2024/CSC 477/Week 2/airquality_datasets.csv')\n# Summary of the dataset\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n\n\npng\n\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\npng\n\n\n\n\n4.3.2 Week 5\n\n\n4.3.3 1. Introduction to Plotnine\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\n\n# !pip install plotnine\n\n\n\n4.3.4 2. The Grammar of Graphics\nThe Grammar of Graphics consists of the following key components:\n\nData: The data you want to visualize.\nAesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc.\nGeometries (geom): The type of plot, like points, lines, bars, etc.\nFacets: Subplots based on the data.\nScales: Control the mapping from data to aesthetic properties.\nCoordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.).\nThemes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n\n\n4.3.5 3. Creating Your First Plot\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a basic scatter plot\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.3.6 4. Adding Aesthetic Mappings\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\nExample: Color by cyl (number of cylinders)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\n\n\n\n\nExample: Size by horsepower (hp)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\n\n\n\n\n\n\n\n\n\n4.3.7 5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\nExample: Adding a smooth line (geom_smooth)\n\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.3.8 6. Faceting\nFaceting allows you to split your plot into multiple panels based on a factor.\nExample: Facet by cyl\n\nfrom plotnine import facet_wrap\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.3.9 7. Customizing Scales\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\nExample: Custom Color Scale\n\nfrom plotnine import scale_color_manual\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\n\n\n\n\n\n\n4.3.10 8. Flip Coordinates\nCreate a bar plot showing distribution of cylinders\nExample: Fliping coordinates axis\n\nimport pandas as pd\n\nfrom plotnine import ggplot, aes, geom_bar, coord_flip, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a bar plot showing distribution of cylinders\n(ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n\n\n\n\n\n\n\n\n\n\n4.3.11 9. Themes\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\nExample: Apply a Minimal Theme\n\nfrom plotnine import theme_minimal\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.3.12 10. Saving the Plot\nYou can save your plot using the save method.\nExample: Save the plot\n\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\n\n\nMidterm Report\nData Analysis\n\n\n4.3.13 Load libraries\nimport torch as tch\nimport pandas as pd\nimport scipy as sci\nimport openpyxl as opxl\nimport seaborn as sns\n\n\n4.3.14 Load CSV\ndf = pd.read_csv('data.csv', low_memory=False)\n\n\n4.3.15 Summary Stats\ndf.head()\n\n\n\n\n\n\n\n\nSCRMCTRL\n\n\nPPCSWGT\n\n\nSEQNUM\n\n\nSEX\n\n\nAGE\n\n\nINTTYPE\n\n\nNONINT\n\n\nHISP\n\n\nMODE\n\n\nPSSTRATA\n\n\n…\n\n\nV352A\n\n\nV352B\n\n\nV352C\n\n\nV352D\n\n\nV352E\n\n\nV352F\n\n\nCHECK_ITEM_J\n\n\nCHECK_ITEM_K\n\n\nCHECK_ITEM_L\n\n\nV353\n\n\n\n\n\n\n0\n\n\n2.030020e+18\n\n\n4150.904836\n\n\n1.0\n\n\n\nMale\n\n\n\n45-64\n\n\n\nPPCS Interview - Telephone\n\n\nNaN\n\n\n\nWhite Only\n\n\n\nComputer-assisted personal interviewing\n\n\n54\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n1\n\n\n2.030030e+18\n\n\n1601.829088\n\n\n2.0\n\n\n\nFemale\n\n\n\n25-44\n\n\n\nPPCS Interview - Telephone\n\n\nNaN\n\n\n\nWhite Only\n\n\n\nComputer-assisted personal interviewing\n\n\n8\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n2.030030e+18\n\n\n0.000000\n\n\n3.0\n\n\n\nMale\n\n\n\n16-17\n\n\n\nPPCS Noninterview\n\n\n\nNCVS Interview Completed by Proxy\n\n\n\nBlack Only\n\n\n\nComputer-assisted personal interviewing\n\n\n8\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n3\n\n\n2.030030e+18\n\n\n0.000000\n\n\n4.0\n\n\n\nMale\n\n\n\n25-44\n\n\n\nPPCS Noninterview\n\n\n\nNCVS Interview Completed by Proxy\n\n\n\nWhite Only\n\n\n\nComputer-assisted personal interviewing\n\n\n8\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\n2.030030e+18\n\n\n1672.290183\n\n\n5.0\n\n\n\nMale\n\n\n\n25-44\n\n\n\nPPCS Interview - Telephone\n\n\nNaN\n\n\n\nBlack Only\n\n\n\nComputer-assisted personal interviewing\n\n\n37\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n5 rows × 266 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.16 Mapping Race (HISP) and CHECK_ITEM_L\nCHECK_ITEM_L is the survey question asked to participants on whether or not they have been arrested before\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP'])\n\n# Create a count plot\nplt.figure(figsize=(12, 6))\nsns.countplot(data=df_filtered, x='HISP', hue='CHECK_ITEM_L', palette='viridis')\n\n# Add titles and labels\nplt.title('Count of CHECK_ITEM_L by HISP', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels\nunique_labels = df_filtered['CHECK_ITEM_L'].dropna().unique()\nlabel_names = {1: 'Yes', 2: '2', 9: 'Out of universe'}\nplt.legend(title='CHECK_ITEM_L', labels=[label_names.get(label, str(label)) for label in unique_labels])\n\nplt.grid(axis='y')\nplt.show()\n\n\n\npng\n\n\nThis plot, generated from the 2018 Police-Public Contact Survey, displays the count of interactions where respondents answered “Yes” to a specific question (CHECK_ITEM_L), categorized by race (HISP). The survey investigates civilian experiences with law enforcement. The highest count is observed among White individuals, which is consistent with their larger representation in the dataset. In contrast, racial groups such as Asians and others have significantly fewer “Yes” responses. This chart highlights racial differences in certain law enforcement-related interactions, suggesting possible disparities in how different groups experience or report these encounters.\n\n\n4.4 Race arrests by SEX with SCRMCTRL as Count Variable\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# SEX Mapping\nsex_mapping = {\n    '(1) Male': 1,   # Male\n    '(2) Female': 2, # Female\n}\n\ndf['SEX'] = df['SEX'].astype(str).map(sex_mapping)\ndf['SEX'] = pd.to_numeric(df['SEX'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP', 'SEX', 'SCRMCTRL'])\n\n# Create a count plot with SCRMCTRL as the count variable\nplt.figure(figsize=(12, 6))\n\n# Count the occurrences of SCRMCTRL and use it to plot\nsns.countplot(data=df_filtered, x='HISP', hue='SEX', palette='viridis', dodge=True)\n\n# Add titles and labels\nplt.title('Count of Race arrests by SEX with SCRMCTRL as Count Variable', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count of SCRMCTRL', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels for SEX\nplt.legend(title='SEX', labels=['Male', 'Female'])\n\nplt.grid(axis='y')\n\n# Save the plot as a PNG file\nplt.savefig('arrests_histogram_baruga.png', format='png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n\n\n\npng\n\n\nThis plot, based on the 2018 Police-Public Contact Survey dataset, shows the count of arrests (SCRMCTRL) across different racial categories (HISP), segmented by gender (SEX). The survey, conducted by the U.S. Bureau of Justice Statistics, examines public interactions with law enforcement, such as police stops and arrests. The higher arrest count for White males can be attributed to the fact that White individuals make up the largest racial group in the dataset. The data reveals notable racial and gender disparities in arrests, with arrest counts for females across all racial categories being lower than for males.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#week-4",
    "href": "Baruga_Main.html#week-4",
    "title": "Plotnine Tutorial: Understanding the Grammar of Graphics",
    "section": "4.4 Week 4",
    "text": "4.4 Week 4\n\nDerrick’s Jupyter Notebook: This notebook contains the data cleaning and exploration tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotnine Tutorial: Understanding the Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#markdown-title",
    "href": "Baruga_Main.html#markdown-title",
    "title": "4  Derrick Baruga",
    "section": "4.4 Markdown title",
    "text": "4.4 Markdown title\nMarkdown Lists:\n\nItem 1\nItem 2\nItem 3\n\nEnumarated list\n\nHola\nHi\nNamaste\n\nWe can do bold, or italic\n# Importing Numpy with nickname np\nimport numpy as np\nnp.absolute(-1)\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n[1 2 3 4 5]\n# Lists are native to python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# Dataframes, so we need pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#loading-csv-files",
    "href": "Baruga_Main.html#loading-csv-files",
    "title": "4  Derrick Baruga",
    "section": "4.5 4. Loading csv files",
    "text": "4.5 4. Loading csv files\nTo load .csv files into a ‘DataFrame’, we use pandas function read_csv\ndf = pd.read_csv('/Users/derrickmarkbavaudbaruga/Documents/fall 2024/CSC 477/Week 2/airquality_datasets.csv')\n# Summary of the dataset\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n\n\npng\n\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\npng\n\n\n\n4.5.1 Week 5\n\n\n4.5.2 1. Introduction to Plotnine\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\n\n# !pip install plotnine\n\n\n\n4.5.3 2. The Grammar of Graphics\nThe Grammar of Graphics consists of the following key components:\n\nData: The data you want to visualize.\nAesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc.\nGeometries (geom): The type of plot, like points, lines, bars, etc.\nFacets: Subplots based on the data.\nScales: Control the mapping from data to aesthetic properties.\nCoordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.).\nThemes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n\n\n4.5.4 3. Creating Your First Plot\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a basic scatter plot\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.5.5 4. Adding Aesthetic Mappings\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\nExample: Color by cyl (number of cylinders)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\n\n\n\n\nExample: Size by horsepower (hp)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\n\n\n\n\n\n\n\n\n\n4.5.6 5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\nExample: Adding a smooth line (geom_smooth)\n\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.5.7 6. Faceting\nFaceting allows you to split your plot into multiple panels based on a factor.\nExample: Facet by cyl\n\nfrom plotnine import facet_wrap\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.5.8 7. Customizing Scales\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\nExample: Custom Color Scale\n\nfrom plotnine import scale_color_manual\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\n\n\n\n\n\n\n4.5.9 8. Flip Coordinates\nCreate a bar plot showing distribution of cylinders\nExample: Fliping coordinates axis\n\nimport pandas as pd\n\nfrom plotnine import ggplot, aes, geom_bar, coord_flip, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a bar plot showing distribution of cylinders\n(ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n\n\n\n\n\n\n\n\n\n\n4.5.10 9. Themes\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\nExample: Apply a Minimal Theme\n\nfrom plotnine import theme_minimal\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.5.11 10. Saving the Plot\nYou can save your plot using the save method.\nExample: Save the plot\n\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\n\n\nMidterm Report\nData Analysis\n\n\n4.5.12 Load libraries\nimport torch as tch\nimport pandas as pd\nimport scipy as sci\nimport openpyxl as opxl\nimport seaborn as sns\n\n\n4.5.13 Load CSV\ndf = pd.read_csv('data.csv', low_memory=False)\n\n\n4.5.14 Summary Stats\ndf.head()\n\n\n\n\n\n\n\n\nSCRMCTRL\n\n\nPPCSWGT\n\n\nSEQNUM\n\n\nSEX\n\n\nAGE\n\n\nINTTYPE\n\n\nNONINT\n\n\nHISP\n\n\nMODE\n\n\nPSSTRATA\n\n\n…\n\n\nV352A\n\n\nV352B\n\n\nV352C\n\n\nV352D\n\n\nV352E\n\n\nV352F\n\n\nCHECK_ITEM_J\n\n\nCHECK_ITEM_K\n\n\nCHECK_ITEM_L\n\n\nV353\n\n\n\n\n\n\n0\n\n\n2.030020e+18\n\n\n4150.904836\n\n\n1.0\n\n\n\nMale\n\n\n\n45-64\n\n\n\nPPCS Interview - Telephone\n\n\nNaN\n\n\n\nWhite Only\n\n\n\nComputer-assisted personal interviewing\n\n\n54\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n1\n\n\n2.030030e+18\n\n\n1601.829088\n\n\n2.0\n\n\n\nFemale\n\n\n\n25-44\n\n\n\nPPCS Interview - Telephone\n\n\nNaN\n\n\n\nWhite Only\n\n\n\nComputer-assisted personal interviewing\n\n\n8\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n2.030030e+18\n\n\n0.000000\n\n\n3.0\n\n\n\nMale\n\n\n\n16-17\n\n\n\nPPCS Noninterview\n\n\n\nNCVS Interview Completed by Proxy\n\n\n\nBlack Only\n\n\n\nComputer-assisted personal interviewing\n\n\n8\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n3\n\n\n2.030030e+18\n\n\n0.000000\n\n\n4.0\n\n\n\nMale\n\n\n\n25-44\n\n\n\nPPCS Noninterview\n\n\n\nNCVS Interview Completed by Proxy\n\n\n\nWhite Only\n\n\n\nComputer-assisted personal interviewing\n\n\n8\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\n2.030030e+18\n\n\n1672.290183\n\n\n5.0\n\n\n\nMale\n\n\n\n25-44\n\n\n\nPPCS Interview - Telephone\n\n\nNaN\n\n\n\nBlack Only\n\n\n\nComputer-assisted personal interviewing\n\n\n37\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n5 rows × 266 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.15 Mapping Race (HISP) and CHECK_ITEM_L\nCHECK_ITEM_L is the survey question asked to participants on whether or not they have been arrested before\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP'])\n\n# Create a count plot\nplt.figure(figsize=(12, 6))\nsns.countplot(data=df_filtered, x='HISP', hue='CHECK_ITEM_L', palette='viridis')\n\n# Add titles and labels\nplt.title('Count of CHECK_ITEM_L by HISP', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels\nunique_labels = df_filtered['CHECK_ITEM_L'].dropna().unique()\nlabel_names = {1: 'Yes', 2: '2', 9: 'Out of universe'}\nplt.legend(title='CHECK_ITEM_L', labels=[label_names.get(label, str(label)) for label in unique_labels])\n\nplt.grid(axis='y')\nplt.show()\n\n\n\npng\n\n\nThis plot, generated from the 2018 Police-Public Contact Survey, displays the count of interactions where respondents answered “Yes” to a specific question (CHECK_ITEM_L), categorized by race (HISP). The survey investigates civilian experiences with law enforcement. The highest count is observed among White individuals, which is consistent with their larger representation in the dataset. In contrast, racial groups such as Asians and others have significantly fewer “Yes” responses. This chart highlights racial differences in certain law enforcement-related interactions, suggesting possible disparities in how different groups experience or report these encounters.\n\n\n4.6 Race arrests by SEX with SCRMCTRL as Count Variable\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# SEX Mapping\nsex_mapping = {\n    '(1) Male': 1,   # Male\n    '(2) Female': 2, # Female\n}\n\ndf['SEX'] = df['SEX'].astype(str).map(sex_mapping)\ndf['SEX'] = pd.to_numeric(df['SEX'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP', 'SEX', 'SCRMCTRL'])\n\n# Create a count plot with SCRMCTRL as the count variable\nplt.figure(figsize=(12, 6))\n\n# Count the occurrences of SCRMCTRL and use it to plot\nsns.countplot(data=df_filtered, x='HISP', hue='SEX', palette='viridis', dodge=True)\n\n# Add titles and labels\nplt.title('Count of Race arrests by SEX with SCRMCTRL as Count Variable', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count of SCRMCTRL', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels for SEX\nplt.legend(title='SEX', labels=['Male', 'Female'])\n\nplt.grid(axis='y')\n\n# Save the plot as a PNG file\nplt.savefig('arrests_histogram_baruga.png', format='png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n\n\n\npng\n\n\nThis plot, based on the 2018 Police-Public Contact Survey dataset, shows the count of arrests (SCRMCTRL) across different racial categories (HISP), segmented by gender (SEX). The survey, conducted by the U.S. Bureau of Justice Statistics, examines public interactions with law enforcement, such as police stops and arrests. The higher arrest count for White males can be attributed to the fact that White individuals make up the largest racial group in the dataset. The data reveals notable racial and gender disparities in arrests, with arrest counts for females across all racial categories being lower than for males.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  },
  {
    "objectID": "Toluwanimi_Main.html",
    "href": "Toluwanimi_Main.html",
    "title": "5  Toluwanimi Olufawo",
    "section": "",
    "text": "5.1 Monday",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Toluwanimi Olufawo</span>"
    ]
  },
  {
    "objectID": "Toluwanimi_Main.html#monday",
    "href": "Toluwanimi_Main.html#monday",
    "title": "5  Toluwanimi Olufawo",
    "section": "",
    "text": "5.1.1 Week 1\n\n\n5.1.2 Week 2\n\n\n5.1.3 Week 3\nMonday Tableau Tutorial\nMonday September 9, 2024\nIn Monday’s class, we learned how to use Tableau Public. We began by downloading and installing the software or using the web version. We then imported the dataset (airquality_datasets.csv) and cleaned the data by checking for missing values and renaming columns.\nImporting the Dataset: After opening Tableau Public, I clicked on “Connect to Data,” selected “Text File,” and navigated to the dataset (airquality_datasets.csv). Then I clicked “Open” or dragged and dropped the file.\nVisualization:\nBar Chart for Average Ozone Levels by Month: We dragged ‘Month’ to Rows and ‘Ozone’ to Columns, then aggregated Ozone as an average. If Tableau didn’t automatically create a bar chart, we changed the Marks type to Bar and renamed the sheet tab. Line Chart for Ozone Levels Over Days: In a new worksheet, we dragged ‘Day’ to Columns and ‘Ozone’ to Rows, creating a line chart, and explored properties like color. Scatter Plot for Temperature vs. Ozone: We created a new worksheet, dragged ‘Temp’ to Columns and ‘Ozone’ to Rows. If Tableau defaulted to a Measure, we changed each pill to Dimension. If not showing a scatter plot, we selected Shape in the Marks tab. Map Visualization: Since our dataset didn’t include geographical data like Latitude and Longitude, we skipped this step.\nCreating a Pivot Table: In a new worksheet, we dragged ‘Month’ to Rows, ‘Day’ to Columns, ‘Ozone’ to Color, and ‘Ozone’ to Size. We observed the resulting pivot table and experimented with the added dimensions.\nBuilding a Dashboard: We clicked on the New Dashboard button, dragged and dropped our created sheets onto the dashboard area, arranged them cohesively, and added interactivity using filters and actions. We reviewed the ozone vs. day line plot and considered improvements or fixes.\n\nConclusion\nIn this class , we gained hands-on experience with Tableau Public, from importing and cleaning data to creating a variety of visualizations, including bar charts, line charts, scatter plots, and pivot tables. We also learned how to build interactive dashboards to present our findings in a cohesive way. These skills will be useful for analyzing and visualizing complex datasets, making it easier to draw insights and communicate results effectively.\nMonday Diamond Tableau Tutorial\nSeptember 9, 2024\nIntroduction The Diamonds dataset is widely used for statistical analysis and data visualization, providing information about various attributes of over 50,000 diamonds, such as their carat weight, cut quality, clarity, and price. The aim of this assignment is to clean the dataset, check for any missing values, and create meaningful visualizations using Tableau\nImporting the Dataset:\nSteps to Import the Dataset:\nOpened Tableau and go to File &gt; Open to upload the “Diamonds Dataset (ggplot2).csv.”\nNavigate to File &gt; Open and select the diamonds_ggplot2.csv file to import the data.\nCleaning the Dataset The data set does not have any missing values, but i ensured all the data types are correct. Ensure Data Types Are Correct: Carat: Number (decimal) Cut: String Color: String Clarity: String Depth, Table, Price, X, Y, Z: Numbers\nCreating Visualizations\n1. Bar Chart: Average Price by Cut To explore how diamond prices vary based on their cut quality, I created a bar chart showing the average price for each cut category This chart will show how the average price varies based on the cut quality of the diamonds.\nSteps I Followed: 1.I dragged Cut to the Rows shelf. 2.Then, I dragged Price to the Columns shelf. 3.To see the average price instead of the sum, I right-clicked on Price, selected Measure, and changed it to Average.\nThis gave me a clear view of how diamonds with different cut qualities (Ideal, Premium, etc.) are priced on average.\n\n2.Line Chart: Price Over Carat Next, I wanted to see how the price changes as the carat size increases, so I built a line chart for this.\nSteps I Followed: 1.I dragged Carat to the Columns shelf. 2.Then, I dragged Price to the Rows shelf. 3.To adjust the chart type to a line, I switched the chart type to Line from the Marks card.\nThis line chart helped me visualize how diamonds with larger carat weights tend to have higher prices, showing a clear upward trend.\n\n3. Scatter Plot: Price vs. Carat To get a more detailed look at the relationship between carat weight and price, I created a scatter plot. This allowed me to see the individual diamond data points more clearly.\nSteps I Followed: 1.I dragged Carat to the Columns shelf. 2.Then, I dragged Price to the Rows shelf.\nThis generated a scatter plot that gave me a more granular view of how price is influenced by carat size. The scatter plot revealed the spread of data, showing how prices vary widely for diamonds of similar carat weights.\n\nCreating a Pivot Table After visualizing the data, I wanted to summarize the diamond prices across different cut and color combinations, so I built a pivot table.\nSteps I Followed: 1.I dragged Cut to the Rows shelf. 2.Then, I dragged Color to the Columns shelf. 3.Finally, I dragged Price to Text (or Label) to display the prices. 4.I right-clicked on Price and summarized it by Average to show the average price for each combination of cut and color This pivot table allowed me to easily compare prices across various cut and color categories.\n\nConclusion Through this assignment, I was able to successfully import, clean, and analyze the Diamonds dataset. By ensuring the data was properly cleaned and validated, I created several visualizations, including a bar chart, line chart, and scatter plot. Each provided valuable insights into how different attributes like cut and carat size influence diamond prices. Additionally, I summarized the data using a pivot table, which made it easy to see price variations across different cut and color combinations. Tableau’s flexibility allowed me to explore the dataset interactively, giving me a deeper understanding of the relationships within the data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Toluwanimi Olufawo</span>"
    ]
  },
  {
    "objectID": "Toluwanimi_Main.html#wednesday",
    "href": "Toluwanimi_Main.html#wednesday",
    "title": "5  Toluwanimi Olufawo",
    "section": "5.2 Wednesday",
    "text": "5.2 Wednesday\n\n5.2.1 Week 1\n\n\n5.2.2 Week 2\nSeptember 4 2024\nDuring class today, we learnt about how to create pivot tables using excel and how best to properly present our data to convey the information we found accurately.\nData Description/Selection By using the air-quality dataset, which contains air quality measurements collected from May to September 1973 in New York united states of America. This dataset includes:\n\nOzone\nSolar.R (solar radiation)\nWind (wind speed in mph)\nTemp (temperature in Fahrenheit)\nMonth (May to September)\nDay (1 to 31)\n\nSource\n1.Open the Dataset: The data were sourced from the New York State Department of Conservation (for ozone data) and the National Weather Service (for meteorological data). I obtained an updated version of the airquality.csv file\n2.Identify and Address Missing Values:\nData Cleaning Process: Fill or Remove Missing Values:\nI cleaned the dataset using Excel by removing rows with missing values (NA) and performed exploratory analysis to reveal patterns and trends in ozone levels and other variables.\nVariables to be evaluated In this dataset,i will be using the Temperature and ozone\nStep 2:Creating a PivotTable\nI selected the entire dataset using Ctrl+A, then went to the Insert tab and clicked on PivotTable. I chose to place the PivotTable in a New Worksheet and clicked OK. To set it up, I dragged Month to the Rows area, Day to the Columns area, and Ozone, Solar.R, Wind, and Temp to the Values area. I made sure each value displayed as an Average by clicking the dropdown next to each field in the Values area, selecting Value Field Settings, and choosing Average. Finally, I customized the PivotTable’s design using the Design tab and adjusted the number formatting by right-clicking on the data cells.\n\nStep 3:Creating Visualizations Insert a PivotChart and Customize the Chart: I clicked inside the PivotTable, then went to the Insert tab and selected PivotChart. I chose the chart type that best suited my data, like a Line Chart for trends or a Bar Chart for comparisons, and clicked OK. To customize the chart, I used the Chart Tools to adjust the design, layout, and format, and added essential elements such as titles, axis labels, and legends to enhance readability.\n\nStep 4: Experiment with Different Configurations\nI clicked inside the PivotTable, then went to the Insert tab and selected PivotChart. I chose the chart type that best suited my data, like a Line Chart for trends or a Bar Chart for comparisons, and clicked OK. To customize the chart, I used the Chart Tools to adjust the design, layout, and format, and added essential elements such as titles, axis labels, and legends to enhance readability.\n\nOzone Histogram\n\nThe Ozone distribution shows a right-skewed pattern, indicating that most values are low, with a few high outliers. This could reflect variations in weather or pollution.\nScatter Plot: Ozone vs Temperature\n\nThere is a weak positive correlation between ozone and temperature, suggesting that higher temperatures tend to coincide with higher ozone levels, though other factors like wind or pollution likely contribute.\nAverage Ozone Per Day & Month\n\nThe highest ozone averages occur on May 25th and July 25th, with notable peaks also on May 29th and July 30th. July generally exhibits higher ozone levels, especially around the end of the month.\nSolar Radiation and Temperature Variations\n\nThe chart reveals daily variations in solar radiation and temperature, peaking around days 18, 19, and 29. Both solar radiation and temperature are higher on these days, pointing to intense sunlight and warmth during these periods.\nConclusion\nAfter setting up the PivotTable by selecting the dataset and placing it in a new worksheet, I configured it with Month in the Rows, Day in the Columns, and used Ozone, Solar.R, Wind, and Temp, showing their averages in the Values area. I tweaked the design and number formatting to make it look cleaner. Then, I inserted a PivotChart based on the table, chose the best chart type for my data, and added titles and labels for clarity. To wrap it up, I explored different configurations by switching variables around, like moving Temp to the Axis and Average Ozone to the Values, to see which setup gave the best visual insight into my data. This process helped me organize and present the information in a clear, visually appealing way.\n\n\n5.2.3 Week 3\nWednesday Advanced Tableau Public Tutorial\nSeptember 11,2024\nIn Wednesday’s class, we learned how to implement what was learnt in Tableau Public on Monday at a more advanced level. We began by opening the Tableau public website.\nImporting the Dataset:We then imported the dataset (diamonds_ggplot2_datasets.csv) and cleaned the data by checking for missing values and renaming columns.There weren’t any missing data and NA in the dataset. After opening Tableau Public, I clicked on “Connect to Data,” selected “Text File,” and navigated to the dataset (diamonds_ggplot2_datasets.csv). Then I clicked “Open” or dragged and dropped the file.\nCalculated Fields\nTo enhance my analysis skills, I created a new calculated field in Tableau Public. First, I navigated to Sheet 1, then from the top menu, I selected Analysis and clicked on Create Calculated Field. In the dialog box that appeared, I named the new field Price per Carat. To calculate this, I used the formula [Price] / [Carat]. After entering the formula, I clicked OK, and the new field was added to my dataset, allowing me to view price per carat for each diamond\n\n2.Advanced Chart Types\nNext, I created an advanced chart type, a boxplot, to explore the distribution of diamond prices based on their cut quality. To do this, I first dragged Price to the Rows section and Cut to the Columns section. Then, I opened the Show Me panel on the right side and selected Box Plot. To ensure that the price was treated as a discrete variable, I right-clicked on Price and chose Dimension. To visually distinguish between the different cut categories, I also dragged Cut to the Color shelf. This boxplot now allows me to hover over each category to see details about the price distribution for each cut.\n\n\n\n3. Dual-Axis Charts\nTo compare Price and Carat on a dual-axis chart, I began by dragging Cut to the Columns section. Then, I added Price to the Rows, followed by Carat just below it. Afterward, I right-clicked on the Carat axis and chose Dual-Axis to overlay the two measures on the same chart. To ensure both axes were aligned properly, I right-clicked the axis again and selected Synchronize Axis. Finally, I adjusted the Marks card from Automatic to Line, which allowed me to visualize the relationship between diamond prices and carat weight in a more coherent manner.\n\n\n4. Using Parameters\nTo dynamically filter my dataset based on price, I created a parameter that allows for flexible threshold adjustments. First, I right-clicked in the Data pane and selected Create Parameter. I named it Price Threshold, set the data type to Float, and chose a default value of 500. After clicking OK, I right-clicked the parameter again in the Data pane and chose Show Parameter Control to make it visible on the dashboard.\n\nNext, I created a calculated field by right-clicking in the Data pane and selecting Create Calculated Field. I named this field Price Greater than Threshold, with the formula [Price] &gt; [Price Threshold]. After clicking OK, I applied this calculated field as a filter by dragging it to the Filters shelf. In the Filter Field dialog, I selected True to display only records where the price exceeded the threshold. This setup allows the plot to dynamically update as I adjust the price threshold using the parameter.\n\n\n\nCreating an Interactive Dashboard with Actions\nTo make my dashboard both visually appealing and interactive, I began by dragging and dropping the relevant sheets onto the dashboard to showcase my visualizations. I arranged the sheets thoughtfully to ensure the data story was clear and engaging.\nNext, I added a Filter Action to make the dashboard responsive. I went to the top menu, selected Dashboard, and clicked on Actions. From there, I chose Add Action -&gt; Filter. In the dialog box, I defined the source sheet, which would act as the filter, and the target sheets, which would be filtered based on the selection. I fine-tuned the settings by choosing the filtering behavior—I opted for filtering based on selection, and also ensured that clearing the selection would reset all values.\nTo further enhance interactivity, I added a Highlight Action. Again, I went to Dashboard -&gt; Actions and selected Add Action -&gt; Highlight. I chose which sheets should act as the source and target for the highlights, and customized the settings to highlight specific dimensions that I wanted users to focus on.\nTo personalize the dashboard, I used various tools and techniques to adjust the layout, added titles, and formatted the dashboard to align with my data storytelling goals. I made sure the filters and highlights interacted smoothly with the visualizations by testing the interactivity.\nOnce everything was in place, I published the dashboard by selecting Save to Tableau Public from the File menu. I then shared the link with my class to showcase my work.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Toluwanimi Olufawo</span>"
    ]
  },
  {
    "objectID": "Toluwanimi_Main.html#friday--midterm-projects",
    "href": "Toluwanimi_Main.html#friday--midterm-projects",
    "title": "5  Toluwanimi Olufawo",
    "section": "5.3 Friday- Midterm Projects",
    "text": "5.3 Friday- Midterm Projects\n\n5.3.1 Week 1\n\n\n5.3.2 Week 2\n\n\n5.3.3 Week 3\nWeek3 Friday&Saturday Excel Project Dataset Olufawo_09/13/24\nContinuation of the Geoheritage Sites of the Nation Dataset\nThis week, we are continuing to explore the Geoheritage Sites of the Nation dataset, which offers a detailed inventory of important geological sites across the United States and its territories. The dataset emphasizes locations that are recognized for their scientific, educational, cultural, economic, and aesthetic importance.\nAdvanced Tableau Public Tutorial\nWith the Geoheritage Site datasest anaylsing the dataset even futher we would be doing some more exploartion on the dataset.\nIn this advanced tutorial on the Geoheritage Sites dataset, we will dive deeper into Tableau’s features by exploring calculated fields, advanced chart types, parameters, table calculations, and creating complex, interactive dashboards with action filters for enhanced analysis\nCreate a Calculated Field To begin, open the worksheet (Sheet 1) where the calculated field will be added. Next, navigate to the top menu and select Analysis. From the dropdown menu, choose Create Calculated Field.\nA dialog box will appear where you can provide a name for the calculated field. For instance, if you are calculating total geographical coordinates, you might name it how you prefer , i named it “Total Geo Coordinates.”\n\nIn the formula box, input the desired calculation. For example, to compute the sum of latitude and longitude, I used the formula: SUM(Lat DD) + SUM(Long DD).\nOnce you’ve entered the formula, click OK to save the calculated field. The new field will now appear in the data pane and can be used in your visualizations.\n**Advanced Chart *Types**\nSteps for Advanced Chart Types using Geoheritage Dataset:\nTo explore the geographical data distribution in the Geoheritage dataset, I followed these steps to craft an interactive boxplot and gain deeper insights into the data:\nStep 1: Drag ‘Lat_DD’ (Latitude) to Rows I started by dragging the Lat_DD field into the Rows section. Since this field represents the latitude of each geoheritage site, it forms the foundation of the boxplot, showcasing the spread of geographical data.\n\n\n\n[Step 1: Drag ‘Lat_DD’ to Rows]\n\n\nStep 2: Select ‘TYPE’ as the Categorical Field and Drag to Columns To break down the geographical data, I dragged the TYPE field (representing different site categories) into Columns. This step helped me visualize how latitude varies across different geoheritage site types. You could also use COUNTY or STATE depending on your analysis focus.\n\nStep 3: Create the Boxplot from the ‘Show Me’ Panel From the Show Me panel, I selected the Box Plot option. The boxplot beautifully summarized the latitude distribution across the categories, highlighting key statistical insights.\n\n\n\nStep3\n\n\nStep 4: Converion of ‘Lat_DD’ from a Measure to a Dimension Right-clicking Lat_DD in the Rows shelf, I converted it from a measure to a dimension. This allowed the latitude values to behave as discrete data points within each category, enhancing the clarity of the comparison.\nStep 5: Enhance with Color Coding for Better Clarity After adding TYPE to the Columns shelf, I dragged it to the Color shelf on the Marks card. This color-coding brought the categories to life visually, making it easy to distinguish between them at a glance.\n\n\n\nStep4\n\n\nStep 6: Dive into the Interactive Boxplot Hovering over each boxplot allowed me to see a detailed breakdown of statistics, including the minimum, quartiles, median, and maximum latitude values for each category. The interactivity added a new dimension to my exploration, as I could compare latitudinal variations between site types effortlessly.\nThis approach gave me a dynamic and visual understanding of how geographical data varies across different categories within the Geoheritage dataset.\nDual-Axis Charts\nDual-axis charts are a powerful tool for visualizing two different measures on separate scales within the same graph. This allows for a clear comparison of how these measures vary across categories in the dataset. The following steps detail the creation of a dual-axis chart using the Geoheritage dataset.\nStep-by-Step Process: Drag ‘TYPE’ to the Columns shelf Begin by dragging the TYPE field into the Columns shelf. This field categorizes the data by geoheritage site type, and it will be used to organize the chart along the x-axis.\nDrag ‘Lat_DD’ (Latitude) to the Rows shelf Next, drag the Lat_DD field into the Rows shelf. This will be the first numerical measure plotted on the y-axis. Latitude provides geographic data, allowing us to analyze the spatial spread of the geoheritage sites.\n\n\n\nStep1\n\n\nDrag ‘AESTHETIC’ to the Rows shelf (below ‘Lat_DD’) Now, drag the AESTHETIC field to the Rows shelf, placing it directly below the Lat_DD field. This step creates a second y-axis, which will represent the aesthetic value assigned to each geoheritage site.\nRight-click on the second axis (AESTHETIC) and select ‘Dual-Axis’ To overlay both y-axes on the same graph, right-click on the axis for AESTHETIC and select Dual-Axis. This allows both Lat_DD and AESTHETIC to be displayed on the same chart, making it easier to compare the geographical spread against the aesthetic value.\nChange the aggregation to ‘Average’ Right-click on each measure in the Rows shelf (Lat_DD and AESTHETIC), and from the options that appear, choose Measure &gt; Average. This ensures that the chart displays the average values for each measure across the different geoheritage site types.\n\n\n\nStep2\n\n\nChange the Marks type to ‘BAR’ For a more interpretable visualization, switch the Marks type to BAR. Click on the All section in the Marks card, and from the drop-down menu, select Line. This step will represent the trends of both latitude and aesthetic value using smooth, continuous lines.\n Creating Complex Dashboards Dashboards in Tableau allow you to combine multiple visualizations, and dashboard actions add interactivity through filters, highlights, and navigation.\nStep-by-Step Process: Create a New Dashboard\nClick the New Dashboard button at the bottom of the workbook. Drag and drop your sheets (e.g., the dual-axis chart, boxplot) onto the dashboard. Add Dashboard Actions\nGo to the Dashboard menu and select Actions. Click Add Action and choose from Filter, Highlight, or URL to add interactivity. Use the previously created filters, or configure the action by selecting the source and target sheets. Click OK to apply the action.\n Here’s a personalized conclusion for your report:\nConclusion\nWith the use of advanced features like calculated fields, dual-axis charts, parameters, table calculations, and dashboard actions, I have developed a more interactive and insightful dashboard using the Geoheritage dataset. These tools greatly enhanced my data analysis, allowing for deeper exploration of geoheritage sites and their characteristics. This approach provides a clearer understanding of the data, enabling more engaging and informative visualizations. I’m excited to continue exploring and analyzing data using Tableau Public!\nBelow is the link to my DASHBOARD\nVisit my Tableau Dashboard\n\n\n\nDashboard\n\n\n\n\n5.3.4 Week 5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Toluwanimi Olufawo</span>"
    ]
  },
  {
    "objectID": "Toluwanimi_Main.html#jupyter-notebook",
    "href": "Toluwanimi_Main.html#jupyter-notebook",
    "title": "5  Toluwanimi Olufawo",
    "section": "5.4 Jupyter Notebook",
    "text": "5.4 Jupyter Notebook\n\n5.4.1 Week 4\n\nToluwanimi’s Notebook: This notebook contains the data cleaning and exploration tasks.\n\n#This is a markdown title\nIn Markdown, we can create lists: - item 1 - item 2 - item 3\nAlso, we can create enumerated lists: 1. Hola 2. Hi 3. Namaste\nWe can bold text, and also italicize text.\n# Here we are importing numpy with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint (arr)\n1\n[1 2 3 4 5]\n# lists are native to python \nmy_list = [1, 2, 3, 4, 5]\nprint (my_list)\n[1, 2, 3, 4, 5]\n#We will be using a lot of dataframes , so we need pandas libary \nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n#load .csv files into **DataFrame**, we use pandas function **read_csv**\ndf = pd.read_csv(r'C:\\Users\\toluf\\venv477\\Scripts\\airquality_datasets.csv')\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n\n\n\npng\n\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\npng\n\n\n\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\n\n5.4.2 Week 5\nIntroduction to Plotnine\n1. Introduction to Plotnine plotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\n\n#pip install plotnine\n\n2. The Grammar of Graphics The Grammar of Graphics consists of the following key components: - Data: The data you want to visualize. - Aesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc. - Geometries (geom): The type of plot, like points, lines, bars, etc. - Facets: Subplots based on the data. - Scales: Control the mapping from data to aesthetic properties. - Coordinate systems: Adjust how data is projected on the plane -(Cartesian, rotations, polar, etc.). - Themes: Adjust the non-data elements like background, labels, gridlines, etc\n3. Creating Your First Plot Let’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity\n\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a basic scatter plot\nimage1 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\nimage1.save ( \"Scatter Plot of MPG vs Weight.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: Scatter Plot of MPG vs Weight.png\n\n\n4. Adding Aesthetic Mappings In the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more. Example: Color by cyl (number of cylinders)\n\nimage2 = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\nimage2.save (\"MPG vs Weight by Cylinder.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight by Cylinder.png\n\n\nExample: Size by horsepower (hp)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\n\n\n\n\n\n\n\n\nimage3 = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\nimage3.save (\"MPG vs Weight by Cylinder and Horsepower.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight by Cylinder and Horsepower.png\n\n\n5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\nExample: Adding a smooth line (geom_smooth)\n\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\nfrom plotnine import geom_smooth\n\nimage4= (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\nimage4.save (\"MPG vs Weight with Regression Line.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight with Regression Line.png\n\n\n\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\nfrom plotnine import geom_smooth\n\nimage5 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\nimage5.save (\"MPG vs Weight with Regression Line.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight with Regression Line.png\n\n\n\nfrom plotnine import ggplot, aes, geom_point, geom_smooth, labs\nfrom plotnine.data import mtcars\n\n# Create the plot\n(\n    ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +  # Corrected parentheses here\n    geom_point() +\n    geom_smooth(method='lm') +  # Linear regression line\n    labs(\n        title='MPG vs Weight with Regression Line',\n        x='Weight (1000 lbs)',\n        y='Miles per Gallon'\n    )\n)\n\n\n\n\n\n\n\n\n\nfrom plotnine import ggplot, aes, geom_point, geom_smooth, labs\nfrom plotnine.data import mtcars\n\n# Create the plot\nimage6 = (\n    ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +  # Corrected parentheses here\n    geom_point() +\n    geom_smooth(method='lm') +  # Linear regression line\n    labs(\n        title='MPG vs Weight with Regression Line',\n        x='Weight (1000 lbs)',\n        y='Miles per Gallon'\n    )\n)\nimage6.save (\"MPG vs Weight with Regression Line color.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight with Regression Line color.png\n\n\n6. Faceting Faceting allows you to split your plot into multiple panels based on a factor.\nExample: Facet by cyl\nExample: Facet by cyl\n\nfrom plotnine import facet_wrap\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\nfrom plotnine import facet_wrap\n\nimage7= (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\nimage7.save (\"MPG vs Weight Faceted by Cylinder.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight Faceted by Cylinder.png\n\n\n8. Flip Coordinates Create a bar plot showing distribution of cylinders\nExample: Fliping coordinates axis\n\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_bar, coord_flip, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a bar plot showing distribution of cylinders\n(ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_bar, coord_flip, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a bar plot showing distribution of cylinders\nimage8 = (ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\nimage8.save (\"Distribution of Cylinders.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: Distribution of Cylinders.png\n\n\n9. Themes Themes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\nExample: Apply a Minimal Theme\n\nfrom plotnine import theme_minimal\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\nfrom plotnine import theme_minimal\n\nimage9 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\nimage9.save (\"MPG vs Weight with Minimal Theme.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight with Minimal Theme.png\n\n\n10. Saving the Plot You can save your plot using the save method.\nExample: Save the plot\n\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\n\n\n05_VIZ_python_scikit-learn\nHere’s a structured tutorial to showcase the power of scikit-learn using the Iris dataset, incorporating statistical summaries, visualizations, simple linear regression, correlation plots, and multiple linear regression.\n1. Statistical Summary with Pandas Start by loading the Iris dataset and generating summary statistics using Pandas.\n\nimport pandas as pd\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\n\n# Convert it into a Pandas DataFrame\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Describe the statistics\niris_df.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\nThe .describe() function gives you important statistics, such as count, mean, standard deviation, min, max, and quartiles for each feature\n\nVisualizations Using Matplotlib and Seaborn\n\nNow, visualize the data to understand relationships between features. Seaborn is great for quick visualizations.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Pairplot to visualize relationships between features\nsns.pairplot(iris_df)\nplt.show()\n\n# Add the target (species) to the DataFrame for visualization\niris_df['species'] = iris.target\n\n# Visualize the distribution of each feature using a boxplot\nplt.figure(figsize=(12,6))\nsns.boxplot(data=iris_df.drop(columns='species'))\nplt.title('Feature Distribution')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPairplot helps identify relationships between features.\nBoxplots help visualize the distribution and potential outliers in each feature.\n\n3. Simple Linear Regression We can now build a simple linear regression model. For example, predicting sepal length based on sepal width\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Selecting only Sepal Length and Sepal Width for simple linear regression\nX = iris_df[['sepal width (cm)']]  # Predictor\ny = iris_df['sepal length (cm)']    # Response\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = lin_reg.predict(X_test)\n\n# Visualize the linear regression fit\nplt.scatter(X_test, y_test, color='blue', label='Actual')\nplt.plot(X_test, y_pred, color='red', label='Predicted')\nplt.title('Simple Linear Regression: Sepal Width vs. Sepal Length')\nplt.xlabel('Sepal Width (cm)')\nplt.ylabel('Sepal Length (cm)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nHere we’ve modeled sepal length based on sepal width, split the data into training and test sets, and plotted the predicted vs actual values.\n4. Correlation Plot Next, explore the correlation between features using a heatmap to see how strongly they are related.\n\n# Correlation matrix\ncorr_matrix = iris_df.drop(columns='species').corr()\n\n# Visualize the correlation matrix with a heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix of Iris Features')\nplt.show()\n\n\n\n\n\n\n\n\nThis heatmap will help you understand which features are highly correlated, which can be helpful for model building.\n5. Multiple Linear Regression Finally, let’s perform a multiple linear regression using all the features to predict a response variable (e.g., sepal length).\n\n# All features (excluding target/species) as predictors\nX = iris_df.drop(columns=['species', 'sepal length (cm)'])\ny = iris_df['sepal length (cm)']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the multiple linear regression model\nmlr = LinearRegression()\nmlr.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = mlr.predict(X_test)\n\n# Calculate performance metrics (optional)\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred)}\")\n\n# Visualize actual vs predicted\nplt.scatter(y_test, y_pred)\nplt.title('Multiple Linear Regression: Actual vs Predicted Sepal Length')\nplt.xlabel('Actual Sepal Length (cm)')\nplt.ylabel('Predicted Sepal Length (cm)')\nplt.show()\n\n# Print the equation of the multiple linear regression model\nintercept = mlr.intercept_\ncoefficients = mlr.coef_\n\n# Feature names (excluding the response variable and species)\nfeatures = iris_df.columns.drop(['species', 'sepal length (cm)'])\n\n# Construct the regression equation as a string\nequation = f\"Sepal Length = {intercept:.2f}\"\nfor feature, coef in zip(features, coefficients):\n    equation += f\" + ({coef:.2f} * {feature})\"\n\n# Print the regression equation\nprint(\"The equation for multiple linear regression:\")\nprint(equation)\n\nMean Squared Error: 0.09811742166101381\nR-squared: 0.8525836334296238\n\n\n\n\n\n\n\n\n\nThe equation for multiple linear regression:\nSepal Length = 1.76 + (0.66 * sepal width (cm)) + (0.76 * petal length (cm)) + (-0.69 * petal width (cm))\n\n\nConclusion This tutorial has shown how to: 1. Get a statistical summary of the Iris dataset. 2. Visualize relationships between features using pairplots and boxplots. 3. Perform simple linear regression. 4. Generate a correlation heatmap. 5. Conduct multiple linear regression.\nThese steps give a comprehensive introduction to using scikit-learn along with Pandas, Seaborn, and Matplotlib for exploratory data analysis and modeling.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Toluwanimi Olufawo</span>"
    ]
  },
  {
    "objectID": "Toluwanimi_Main.html#week-4",
    "href": "Toluwanimi_Main.html#week-4",
    "title": "5  Toluwanimi Olufawo",
    "section": "5.5 Week 4",
    "text": "5.5 Week 4\n\nToluwanimi’s Notebook: This notebook contains the data cleaning and exploration tasks.\n\n#This is a markdown title\nIn Markdown, we can create lists: - item 1 - item 2 - item 3\nAlso, we can create enumerated lists: 1. Hola 2. Hi 3. Namaste\nWe can bold text, and also italicize text.\n# Here we are importing numpy with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint (arr)\n1\n[1 2 3 4 5]\n# lists are native to python \nmy_list = [1, 2, 3, 4, 5]\nprint (my_list)\n[1, 2, 3, 4, 5]\n#We will be using a lot of dataframes , so we need pandas libary \nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n#load .csv files into **DataFrame**, we use pandas function **read_csv**\ndf = pd.read_csv(r'C:\\Users\\toluf\\venv477\\Scripts\\airquality_datasets.csv')\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n\n\n\npng\n\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\npng\n\n\n\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Toluwanimi Olufawo</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#week-5",
    "href": "Baruga_Main.html#week-5",
    "title": "Plotnine Tutorial: Understanding the Grammar of Graphics",
    "section": "4.7 Week 5",
    "text": "4.7 Week 5\n\n4.7.1 1. Introduction to Plotnine\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\n\n# !pip install plotnine\n\n\n\n4.7.2 2. The Grammar of Graphics\nThe Grammar of Graphics consists of the following key components:\n\nData: The data you want to visualize.\nAesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc.\nGeometries (geom): The type of plot, like points, lines, bars, etc.\nFacets: Subplots based on the data.\nScales: Control the mapping from data to aesthetic properties.\nCoordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.).\nThemes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n\n\n4.7.3 3. Creating Your First Plot\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a basic scatter plot\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.7.4 4. Adding Aesthetic Mappings\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\nExample: Color by cyl (number of cylinders)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\n\n\n\n\nExample: Size by horsepower (hp)\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\n\n\n\n\n\n\n\n\n\n4.7.5 5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\nExample: Adding a smooth line (geom_smooth)\n\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.7.6 6. Faceting\nFaceting allows you to split your plot into multiple panels based on a factor.\nExample: Facet by cyl\n\nfrom plotnine import facet_wrap\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.7.7 7. Customizing Scales\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\nExample: Custom Color Scale\n\nfrom plotnine import scale_color_manual\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\n\n\n\n\n\n\n4.7.8 8. Flip Coordinates\nCreate a bar plot showing distribution of cylinders\nExample: Fliping coordinates axis\n\nimport pandas as pd\n\nfrom plotnine import ggplot, aes, geom_bar, coord_flip, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\n# Create a bar plot showing distribution of cylinders\n(ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n\n\n\n\n\n\n\n\n\n\n4.7.9 9. Themes\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\nExample: Apply a Minimal Theme\n\nfrom plotnine import theme_minimal\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\n\n\n\n\n\n\n\n4.7.10 10. Saving the Plot\nYou can save your plot using the save method.\nExample: Save the plot\n\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\toluf\\AppData\\Roaming\\Python\\Python312\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotnine Tutorial: Understanding the Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "Tako_Main.html#monday",
    "href": "Tako_Main.html#monday",
    "title": "2  Abigail Tako",
    "section": "",
    "text": "Pce (Personal Consumption Expenditure)\nRepresents the total amount spent by consumers in a year\nPop (Population)\nRepresents the total population for each year\nUempmed (Median Unemployment Duration)\nRepresents the median time people remain unemployed\nUnemploy (Unemployment)\nRepresents the total number of unemployed individuals per year\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrashes\nTotal number of traffic crashes reported\nFatalities\nNumber of individuals killed in traffic-related incidents\nInjured persons\nTotal number of people injured in traffic accidents\nVehicles-miles traveled\nTotal number of miles driven by all vehicles, measured in millions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abigail Tako</span>"
    ]
  },
  {
    "objectID": "Baruga_Main.html#race-arrests-by-sex-with-scrmctrl-as-count-variable",
    "href": "Baruga_Main.html#race-arrests-by-sex-with-scrmctrl-as-count-variable",
    "title": "4  Derrick Baruga",
    "section": "4.4 Race arrests by SEX with SCRMCTRL as Count Variable",
    "text": "4.4 Race arrests by SEX with SCRMCTRL as Count Variable\ndf = pd.read_csv('data.csv', low_memory=False)\n\n# Mapping CHECK_ITEM_L\nCHECK_ITEM_L_mapping = {\n    '(1) Yes': 1,\n    '(2) 2': 2,\n    '(9) Out of universe': 9\n}\n\ndf['CHECK_ITEM_L'] = df['CHECK_ITEM_L'].astype(str).map(CHECK_ITEM_L_mapping)\ndf['CHECK_ITEM_L'] = pd.to_numeric(df['CHECK_ITEM_L'], errors='coerce')\n\n# Mapping HISP\nrace_mapping = {\n    '(1) White Only': 1,\n    '(2) Black Only': 2,\n    '(3) Hispanic': 3,\n    '(4) Asian Only': 4,\n    '(5) Other': 5\n}\n\ndf['HISP'] = df['HISP'].astype(str).map(race_mapping)\ndf['HISP'] = pd.to_numeric(df['HISP'], errors='coerce')\n\n# SEX Mapping\nsex_mapping = {\n    '(1) Male': 1,   # Male\n    '(2) Female': 2, # Female\n}\n\ndf['SEX'] = df['SEX'].astype(str).map(sex_mapping)\ndf['SEX'] = pd.to_numeric(df['SEX'], errors='coerce')\n\n# Drop NaN values for the plot\ndf_filtered = df.dropna(subset=['CHECK_ITEM_L', 'HISP', 'SEX', 'SCRMCTRL'])\n\n# Create a count plot with SCRMCTRL as the count variable\nplt.figure(figsize=(12, 6))\n\n# Count the occurrences of SCRMCTRL and use it to plot\nsns.countplot(data=df_filtered, x='HISP', hue='SEX', palette='viridis', dodge=True)\n\n# Add titles and labels\nplt.title('Count of Race arrests by SEX with SCRMCTRL as Count Variable', fontsize=16)\nplt.xlabel('HISP', fontsize=14)\nplt.ylabel('Count of SCRMCTRL', fontsize=14)\nplt.xticks(ticks=range(len(race_mapping)), labels=race_mapping.keys(), rotation=45)\n\n# Add the legend explicitly with unique labels for SEX\nplt.legend(title='SEX', labels=['Male', 'Female'])\n\nplt.grid(axis='y')\n\n# Save the plot as a PNG file\nplt.savefig('arrests_histogram_baruga.png', format='png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n\n\n\npng\n\n\nThis plot, based on the 2018 Police-Public Contact Survey dataset, shows the count of arrests (SCRMCTRL) across different racial categories (HISP), segmented by gender (SEX). The survey, conducted by the U.S. Bureau of Justice Statistics, examines public interactions with law enforcement, such as police stops and arrests. The higher arrest count for White males can be attributed to the fact that White individuals make up the largest racial group in the dataset. The data reveals notable racial and gender disparities in arrests, with arrest counts for females across all racial categories being lower than for males.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Derrick Baruga</span>"
    ]
  }
]